{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text Classification: Genel bir metin sınıflandırma problemidir. Metinler farklı kategorilere ayrılır. Örneğin, e-postaların spam veya spam olmayan olarak sınıflandırılması.\n",
    "\n",
    "- Textin sayısal özelliklerini örneğin kaç kelime, kaç tane noktalama işareti var gibi ifadeleri ayrı değişken olarak alıp ML ile analiz de edebilirsin.\n",
    "\n",
    "- NLP' de ise texti alıp vektöre dönüştürüyorsun. Sonrasında ise ML ile birleştirip model kuruyorsun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolon isimlerini belirleyerek dosyayı okuma\n",
    "column_names = ['label', 'message']  # Kolon isimleri\n",
    "df = pd.read_csv(r'C:\\Users\\Pc\\Desktop\\SMSSpamCollection.tsv', sep='\\t', header=None, names=column_names, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  I've been searching for the right words to tha...\n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3      ham  Even my brother is not like to speak with me. ...\n",
       "4      ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "...    ...                                                ...\n",
       "5563  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5564   ham               Will ü b going to esplanade fr home?\n",
       "5565   ham  Pity, * was in mood for that. So...any other s...\n",
       "5566   ham  The guy did some bitching but I acted like i'd...\n",
       "5567   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5568 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer Nedir?\n",
    "\n",
    "- TF (Term Frequency): Belirli bir terimin, belgede kaç kez geçtiğini ölçer. Yani, belgedeki toplam kelime sayısına bölünerek terimin o belgede ne kadar yaygın olduğunu gösterir.\n",
    "\n",
    "- IDF (Inverse Document Frequency): Belirli bir terimin kaç belgede geçtiğini ölçer. Sıklıkla geçen kelimelerin (örneğin, \"ve\", \"bu\", \"bir\") daha az önem taşıdığını gösterir. IDF, belirli bir terimin belgelerdeki yaygınlığını azaltarak, nadir kelimelere daha fazla ağırlık verir.\n",
    "\n",
    "TfidfVectorizer'ın Kullanım Amacı:\n",
    "\n",
    "1. Metin Temsili: Metin verilerini sayısal bir forma dönüştürerek, bu verilerin makine öğrenimi algoritmaları tarafından işlenebilmesini sağlar. Her kelime, belgedeki yerleşimine göre bir ağırlık alır.\n",
    "\n",
    "2. Özellik Çıkartma: TfidfVectorizer, metin verilerinden özellikler çıkartır ve bu özellikler, model eğitimi için kullanılabilir.\n",
    "\n",
    "3. Belgelerin Karşılaştırılması: Belirli belgeler arasındaki benzerliği ölçmek için kullanılır. Yüksek TF-IDF değerine sahip kelimeler, belgeler arasındaki farklılıkları ve benzerlikleri anlamaya yardımcı olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3730, 7118)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline Nedir?\n",
    "\n",
    "- Pipeline, makine öğrenimi ve veri işleme süreçlerini düzenlemek için kullanılan bir yapıdır. Özellikle scikit-learn kütüphanesinde yaygın olarak kullanılır. Pipeline, verilerin işlenmesi ve modelin eğitilmesi için bir dizi adımı ardışık bir şekilde gerçekleştirmeyi sağlar. Bu, süreci daha okunabilir ve yönetilebilir hale getirir.\n",
    "\n",
    "Neden Pipeline Kullanılır?\n",
    "\n",
    "1. Kodun Okunabilirliğini Artırma: Birden fazla adımı tek bir yapı içinde tanımlamak, kodun daha temiz ve okunabilir olmasını sağlar.\n",
    "\n",
    "2. Hata Riskini Azaltma: Her adımın düzgün bir şekilde çalışmasını sağlamak için aşamalı bir yapı sunar. Hatalar, daha kolay tanımlanabilir ve yönetilebilir.\n",
    "\n",
    "3. Kolaylıkla Değişiklik Yapabilme: Tek bir adımda değişiklik yapmak, diğer adımları etkilemeden mümkündür.\n",
    "\n",
    "Pipeline Kullanımında Temel Adımlar\n",
    "Aşağıdaki adımlar, Pipeline kullanırken yaygın olarak izlenir:\n",
    "\n",
    "* Veri Ön İşleme: Verilerin uygun formata getirilmesi, eksik verilerin işlenmesi, kategorik verilerin dönüştürülmesi vb.\n",
    "* Özellik Çıkartma: TfidfVectorizer, CountVectorizer gibi yöntemlerle metin verilerinden özellikler çıkartma.\n",
    "* Model Seçimi: Kullanılacak makine öğrenimi modelinin belirlenmesi (örneğin, LinearSVC, RandomForestClassifier, vb.).\n",
    "* Model Eğitimi: Modelin eğitim verileri üzerinde eğitilmesi.\n",
    "* Model Değerlendirme: Modelin test verileri üzerindeki performansının değerlendirilmesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, LinearSVC())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # CountVectorizer için kısaltma\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1608\n",
      "        spam       0.99      0.90      0.94       230\n",
      "\n",
      "    accuracy                           0.99      1838\n",
      "   macro avg       0.99      0.95      0.97      1838\n",
      "weighted avg       0.99      0.99      0.99      1838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yukardaki tf-idfvectorizer ile vektör oluşturup predictiondı.\n",
    "\n",
    "- Aşağıdaki ise spacy ile vektör oluşturup 300 boyutlu her bir satır için o şekilde predictiondı. Ama aynı sonucu elde ettim garip bir şekilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF ve SpaCy'nin word embedding tabanlı vektörleştirme yöntemleri birbirinden farklı prensiplere dayanır, bu yüzden sonuçların birebir aynı olması genellikle beklenmez. Ancak belirli durumlarda, bu iki yöntemden elde edilen vektörlerin sınıflandırma modelinde (örneğin SVM) aynı sonuçlara yol açması mümkündür. İşte neden böyle olabileceğine dair birkaç açıklama:\n",
    "\n",
    "1. Verinin Yapısı ve Temsili:\n",
    "- TF-IDF: Kelimeleri belge içinde geçiş sıklığına ve nadirliğine göre vektörleştirir. Genellikle, daha fazla geçen ve önemli olan kelimelere yüksek ağırlık verir.\n",
    "- Word Embeddings (SpaCy): Kelimeleri anlamlarına göre yerleştirir ve aynı anlam çevresine sahip kelimeler benzer vektörler üretir.\n",
    "Eğer verinizdeki metinler, hem TF-IDF hem de word embedding'ler için belirgin ve tutarlı bir desen oluşturuyorsa, her iki yöntemle oluşturulan vektörler SVM tarafından aynı şekilde sınıflandırılabilir. Özellikle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy modelini yükle\n",
    "nlp = spacy.load('en_core_web_md')  # Büyük bir model kullanın (örneğin 'en_core_web_md')\n",
    "\n",
    "X_train_nlp = []\n",
    "\n",
    "# Her bir metni işle ve vektörü oluştur\n",
    "for i in range(X_train.shape[0]):\n",
    "    doc = nlp(X_train.iloc[i])  # Pandas Series ise iloc ile satıra erişilir\n",
    "    \n",
    "    # Sadece alfabetik ve stop-word olmayan kelimelerin vektörlerini kullan\n",
    "    vectors = [token.vector for token in doc if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    # Eğer vektörler boşsa, sıfır vektörünü ekleyin (örneğin SpaCy modelinin boyutunda)\n",
    "    if len(vectors) > 0:\n",
    "        vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros(nlp.vocab.vectors_length)  # SpaCy'nin vektör boyutu\n",
    "    \n",
    "    X_train_nlp.append(vector)\n",
    "\n",
    "# X_train'i numpy dizisine dönüştür\n",
    "X_train = np.array(X_train_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kelime Vektörleri (Embedding ile Ortalama Almak)\n",
    "- Bu yöntem, kelimeleri anlamlarına göre vektör uzayında yerleştiren kelime vektörlerini (embeddings) kullanır. Örneğin, bir satırdaki her kelimenin vektörünü alıp ortalama aldığınızda, o satırın genel \"anlamını\" veya \"içeriğini\" tek bir vektörde özetlemiş olursunuz. Kelime vektörleri genellikle SpaCy, Word2Vec, GloVe gibi pre-trained modellerden elde edilir.\n",
    "\n",
    "Farkı: \n",
    "\n",
    "- Kelime vektörleri kelimenin anlamını taşır ve bağlama göre anlamlandırılır. Anlam olarak yakın kelimeler (örneğin \"kral\" ve \"kraliçe\") vektör uzayında birbirine yakın yerleştirilir. Dolayısıyla, metnin içerdiği anlamlar, o metindeki kelimelerin vektörlerinin ortalamasıyla temsil edilebilir.\n",
    "\n",
    "Avantajları:\n",
    "\n",
    "- Anlamsal benzerlikleri yakalar: \"mutlu\" ve \"sevinçli\" gibi kelimeler farklı olsa da benzer anlamlara sahip oldukları için bu yöntemle vektörleri yakın olur.\n",
    "- Anlam, bağlam ve ilişkileri hesaba katar. Anlamsal ilişkilere dayalı olarak daha sofistike sonuçlar verebilir.\n",
    "- Genelleme kapasitesi daha yüksektir, çünkü aynı anlam çevresindeki kelimeler benzer temsil edilir.\n",
    "\n",
    "Dezavantajları:\n",
    "\n",
    "- Bağlama özgü ayrıntılar kaybolabilir: Örneğin, çok karmaşık cümlelerde veya ironik ifadelerde, ortalama almak, bağlamın bazı önemli detaylarını silebilir.\n",
    "- Hesaplama maliyeti yüksek olabilir, çünkü vektörler genellikle büyük boyutludur (örn. 300 boyutlu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy modelini yükle\n",
    "nlp = spacy.load('en_core_web_md')  # Büyük bir model kullanın (örneğin 'en_core_web_md')\n",
    "\n",
    "X_test_nlp = []\n",
    "\n",
    "# Her bir metni işle ve vektörü oluştur\n",
    "for i in range(X_test.shape[0]):\n",
    "    doc = nlp(X_test.iloc[i])  # Pandas Series ise iloc ile satıra erişilir\n",
    "    \n",
    "    # Sadece alfabetik ve stop-word olmayan kelimelerin vektörlerini kullan\n",
    "    vectors = [token.vector for token in doc if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    # Eğer vektörler boşsa, sıfır vektörünü ekleyin (örneğin SpaCy modelinin boyutunda)\n",
    "    if len(vectors) > 0:\n",
    "        vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros(nlp.vocab.vectors_length)  # SpaCy'nin vektör boyutu\n",
    "    \n",
    "    X_test_nlp.append(vector)\n",
    "\n",
    "# X_test'i numpy dizisine dönüştür\n",
    "X_test = np.array(X_test_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      0.95      0.96      1608\n",
      "        spam       0.70      0.73      0.71       230\n",
      "\n",
      "    accuracy                           0.93      1838\n",
      "   macro avg       0.83      0.84      0.84      1838\n",
      "weighted avg       0.93      0.93      0.93      1838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment analysis için spacy ile vektör oluşturup 300 boyutlu bu şekilde analiz yapabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')  # make sure to use a larger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.76275  , -0.2074   ,  0.32124  , -0.45817  , -0.36041  ,\n",
       "        0.60152  ,  0.74492  ,  0.079701 , -0.23428  ,  0.31224  ,\n",
       "       -0.43189  ,  0.20746  , -0.40774  , -0.2538   , -0.12308  ,\n",
       "       -0.39652  ,  0.2361   ,  0.20473  ,  0.49645  ,  0.26983  ,\n",
       "       -0.45623  ,  0.20879  , -0.0027486,  0.050435 , -0.003348 ,\n",
       "       -0.088146 , -0.53855  ,  0.42507  , -0.70535  , -0.31984  ,\n",
       "       -0.56023  , -0.042877 ,  0.31083  , -0.23038  ,  0.43657  ,\n",
       "        0.18436  ,  0.52997  , -0.1038   ,  0.010579 ,  0.5373   ,\n",
       "       -0.64911  ,  0.15171  , -0.13305  , -0.17798  ,  0.083277 ,\n",
       "       -0.15615  , -0.1446   , -0.27032  ,  0.19693  ,  0.22867  ,\n",
       "       -0.5745   ,  0.16468  ,  0.029028 ,  0.66735  , -0.13859  ,\n",
       "        0.51194  , -0.029928 ,  0.11792  , -0.15589  ,  0.69151  ,\n",
       "        0.19838  ,  0.032428 ,  0.37526  ,  0.099998 , -0.48631  ,\n",
       "       -0.13605  ,  0.5295   , -0.089214 ,  0.34458  ,  0.19393  ,\n",
       "        0.012607 , -0.1999   , -0.38105  , -0.05991  , -0.81039  ,\n",
       "        0.16744  , -0.71753  , -0.45249  , -0.15331  , -0.63738  ,\n",
       "       -0.65694  , -0.18735  ,  0.63503  ,  0.21831  ,  0.25238  ,\n",
       "       -0.28389  ,  0.92852  ,  0.27839  ,  0.23513  , -0.17551  ,\n",
       "        0.27253  , -0.069459 ,  0.28146  , -0.72036  , -0.21074  ,\n",
       "        0.2844   , -0.56551  , -0.088767 , -0.53687  ,  0.32924  ,\n",
       "        0.018555 ,  0.44599  ,  0.53125  ,  0.33813  , -0.2496   ,\n",
       "       -1.3837   , -0.6186   ,  0.16952  ,  0.41993  , -0.14406  ,\n",
       "        0.49195  , -0.44095  ,  0.22829  , -0.3835   ,  0.37375  ,\n",
       "        0.017091 ,  0.7226   , -0.059622 ,  0.18684  ,  0.38005  ,\n",
       "        0.49409  ,  0.048001 ,  0.64468  , -0.3216   , -0.36432  ,\n",
       "       -0.073444 , -0.016888 , -0.33217  , -0.87165  , -0.096356 ,\n",
       "        0.098888 , -0.8651   ,  0.0093075,  0.27166  ,  0.088393 ,\n",
       "       -0.39287  , -0.043349 , -0.1348   ,  0.25003  , -0.042534 ,\n",
       "       -1.6471   ,  0.13621  , -0.09254  ,  0.0056756,  0.73089  ,\n",
       "       -0.60677  , -0.098333 ,  0.31013  ,  0.24706  , -0.04379  ,\n",
       "       -0.32615  , -0.066207 ,  0.14114  , -0.77077  , -0.29903  ,\n",
       "        0.23375  , -0.34266  ,  0.39098  ,  0.57531  , -0.10536  ,\n",
       "        0.037208 ,  0.13038  , -0.5205   , -0.069537 ,  0.15813  ,\n",
       "        0.32067  , -0.26295  , -0.22167  ,  0.0075272,  0.20216  ,\n",
       "        0.026607 , -0.074615 ,  0.080685 ,  0.048114 , -0.66353  ,\n",
       "        0.22107  ,  0.24209  , -0.1021   ,  0.305    ,  0.14341  ,\n",
       "       -0.054846 , -0.11603  , -0.24537  ,  0.27788  , -0.2664   ,\n",
       "       -0.3657   ,  0.14288  , -0.73955  , -0.2029   , -0.46982  ,\n",
       "       -0.2818   ,  0.55073  , -0.20431  ,  0.46961  ,  0.14369  ,\n",
       "        0.25199  ,  0.54011  , -0.44682  ,  0.22835  , -0.56032  ,\n",
       "       -0.563    ,  0.28263  , -0.36653  ,  0.0044703,  0.41113  ,\n",
       "        0.45625  ,  0.2533   ,  0.10798  , -0.09376  , -0.47141  ,\n",
       "        0.074787 ,  0.6536   ,  0.11043  ,  0.33477  , -0.59218  ,\n",
       "       -0.45336  ,  0.15047  ,  0.70703  ,  0.31043  ,  0.48735  ,\n",
       "        0.36108  ,  0.027264 ,  0.01392  , -0.1515   , -0.29762  ,\n",
       "        0.17523  ,  0.96212  ,  0.36939  ,  0.26589  ,  0.10913  ,\n",
       "       -0.03072  , -0.23175  , -0.4      , -0.095803 , -0.024775 ,\n",
       "        0.14012  , -0.72798  , -0.13216  ,  0.39832  ,  0.030282 ,\n",
       "       -0.047631 , -0.060496 ,  0.12715  , -0.86461  , -0.20217  ,\n",
       "        0.32554  , -0.11635  , -0.099599 , -0.048509 , -0.45854  ,\n",
       "        0.098873 , -0.27002  , -0.25829  ,  0.10591  ,  0.38639  ,\n",
       "        0.077101 ,  0.5914   ,  0.32764  ,  0.15978  ,  0.081    ,\n",
       "        0.07582  ,  0.32709  , -0.015143 ,  0.2692   ,  0.14536  ,\n",
       "        0.49708  ,  0.20422  , -0.30185  , -0.74162  ,  0.28929  ,\n",
       "        0.29051  ,  0.026477 ,  0.23335  ,  0.11181  ,  0.42294  ,\n",
       "        0.72279  ,  0.33024  , -0.37637  ,  0.13693  , -0.50228  ,\n",
       "        0.062155 ,  0.43698  , -0.18967  , -0.43764  ,  0.44629  ,\n",
       "       -0.1434   ,  0.064268 , -0.56959  , -0.066664 , -0.46406  ,\n",
       "       -0.0044868, -0.062906 , -0.86369  , -0.16995  , -0.62189  ,\n",
       "        0.55016  ,  0.09481  , -0.042774 ,  0.17285  , -0.55231  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'lion').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 300)\n"
     ]
    }
   ],
   "source": [
    "# Örnek metinler\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Vektörleri saklamak için bir liste\n",
    "X_train = []\n",
    "\n",
    "# Her bir metni işle ve vektörü oluştur\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    # Kelimelerin vektörlerini toplamak için bir numpy dizisi oluştur\n",
    "    vector = np.mean([token.vector for token in doc if token.has_vector], axis=0)   # birbirine anlam olarak yakın kelimelerin vektörleri de benzer. o yüzden ortalama alınıp her cümle için tek vektör oluyor\n",
    "    X_train.append(vector)\n",
    "\n",
    "# X_train'i numpy dizisine dönüştür\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# X_train'in boyutunu kontrol et\n",
    "print(X_train.shape)  # (4, 300) gibi bir çıktı alırsınız, modelin vektör boyutuna bağlı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ya da \"NLTK's VADER module\" ile daha sağlam bir analiz yapmış oluruz.\n",
    "\n",
    "Vader_lexicon'ın Özellikleri\n",
    "1. Kelime Duyguları: vader_lexicon, belirli kelimelerin duygusal değerlerini belirten bir kelime listesi içerir. Bu listede, kelimelerin pozitif, negatif veya nötr duygusal tonları belirtilmiştir.\n",
    "\n",
    "2. Duygusal Ağırlık: Her kelime, duygu değeri (valence) ile birlikte gelir. Örneğin, \"love\" kelimesi pozitif bir değere sahipken, \"hate\" kelimesi negatif bir değere sahiptir.\n",
    "\n",
    "3. Duygu Hesaplama: VADER, metinlerin toplam duygusal değerini hesaplamak için bu kelime listesini kullanır. Bir metindeki kelimelerin duygusal değerleri toplanarak genel duygu skoru elde edilir.\n",
    "\n",
    "4. Negatif ve Pozitif Duygu: Ayrıca, VADER, metinlerdeki duygu ifade biçimlerini (örneğin, büyük harfle yazma veya emojiler) dikkate alarak daha doğru sonuçlar verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Pc\\Desktop\\amazonreviews.tsv', sep='\\t', header=0, names=column_names, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>pos</td>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>neg</td>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>neg</td>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      pos  Stuning even for the non-gamer: This sound tra...\n",
       "1      pos  The best soundtrack ever to anything.: I'm rea...\n",
       "2      pos  Amazing!: This soundtrack is my favorite music...\n",
       "3      pos  Excellent Soundtrack: I truly like this soundt...\n",
       "4      pos  Remember, Pull Your Jaw Off The Floor After He...\n",
       "...    ...                                                ...\n",
       "9995   pos  A revelation of life in small town America in ...\n",
       "9996   pos  Great biography of a very interesting journali...\n",
       "9997   neg  Interesting Subject; Poor Presentation: You'd ...\n",
       "9998   neg  Don't buy: The box looked used and it is obvio...\n",
       "9999   pos  Beautiful Pen and Fast Delivery.: The pen was ...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE NaN VALUES AND EMPTY STRINGS:\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "\n",
    "df.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'compound': 0.9454}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(df.loc[0]['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['puan'] = df['message'].apply(lambda message: sid.polarity_scores(message)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['puan'].apply(lambda puan: 'pos' if puan>=0 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>puan</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>pos</td>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>neg</td>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "      <td>0.9102</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>neg</td>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "      <td>-0.3595</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message    puan  \\\n",
       "0      pos  Stuning even for the non-gamer: This sound tra...  0.9454   \n",
       "1      pos  The best soundtrack ever to anything.: I'm rea...  0.8957   \n",
       "2      pos  Amazing!: This soundtrack is my favorite music...  0.9858   \n",
       "3      pos  Excellent Soundtrack: I truly like this soundt...  0.9814   \n",
       "4      pos  Remember, Pull Your Jaw Off The Floor After He...  0.9781   \n",
       "...    ...                                                ...     ...   \n",
       "9995   pos  A revelation of life in small town America in ...  0.9610   \n",
       "9996   pos  Great biography of a very interesting journali...  0.9544   \n",
       "9997   neg  Interesting Subject; Poor Presentation: You'd ...  0.9102   \n",
       "9998   neg  Don't buy: The box looked used and it is obvio... -0.3595   \n",
       "9999   pos  Beautiful Pen and Fast Delivery.: The pen was ...  0.9107   \n",
       "\n",
       "     sentiment  \n",
       "0          pos  \n",
       "1          pos  \n",
       "2          pos  \n",
       "3          pos  \n",
       "4          pos  \n",
       "...        ...  \n",
       "9995       pos  \n",
       "9996       pos  \n",
       "9997       pos  \n",
       "9998       neg  \n",
       "9999       pos  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7097"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['label'],df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.86      0.52      0.64      5097\n",
      "         pos       0.64      0.91      0.75      4903\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.75      0.71      0.70     10000\n",
      "weighted avg       0.75      0.71      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['label'],df['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp.max_length = 1198623\n",
    "\n",
    "# nlp = spacy.load('en', disable=['parser', 'tagger', 'ner']):\n",
    "\n",
    "# spacy.load('en') kısmı, İngilizce için bir dil modeli yükler. 'en' burada spaCy'nin önceden eğitilmiş İngilizce modelini ifade eder.\n",
    "# disable=['parser', 'tagger', 'ner'] argümanı, yüklenen modelin belirli bileşenlerini devre dışı bırakır:\n",
    "# parser: Cümleleri yapısal olarak analiz eden bileşen (sözdizimi analizi).\n",
    "# tagger: Kelimelere etiket atayan bileşen (part-of-speech tagging).\n",
    "# ner: Adlandırılmış varlık tanıma bileşeni (kişiler, yerler, kuruluşlar vb. tanımak için).\n",
    "# Bu bileşenleri devre dışı bırakmak, işlem süresini kısaltabilir ve bellek kullanımını azaltabilir, böylece sadece temel dil işleme işlevlerine odaklanmanıza olanak tanır.\n",
    "\n",
    "# nlp.max_length = 1198623:\n",
    "\n",
    "# nlp.max_length, spaCy'nin işlemleyebileceği maksimum dizi uzunluğunu ayarlar. Varsayılan olarak, spaCy, çok uzun metinlerle başa çıkma konusunda kısıtlamalara sahip olabilir.\n",
    "# 1198623 değeri, modelin işleyebileceği maksimum karakter sayısını temsil eder. Bu, özellikle büyük metin parçalarıyla çalışırken yararlıdır. Örneğin, bir modelin 1 milyon karakterden uzun bir metni analiz etmesi gerektiğinde, bu değer artırılarak ayarlanabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "\n",
    "# if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_{|}~\\t\\n '`:\n",
    "# Bu koşul, token'ın metninin belirli karakterler (boşluk, yeni satır, özel karakterler vb.) içermediğini kontrol eder.\n",
    "# Eğer token bu karakterler dışında bir metne sahipse, o zaman o token küçük harfe dönüştürülerek listeye eklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file(r'C:\\Users\\Pc\\Desktop\\moby_dick_four_chapters.txt')\n",
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on',\n",
       " 'shore',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'i',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'about',\n",
       " 'a',\n",
       " 'little',\n",
       " 'and',\n",
       " 'see',\n",
       " 'the',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'i',\n",
       " 'have',\n",
       " 'of',\n",
       " 'driving',\n",
       " 'off',\n",
       " 'the',\n",
       " 'spleen',\n",
       " 'and',\n",
       " 'regulating',\n",
       " 'the',\n",
       " 'circulation',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'find',\n",
       " 'myself',\n",
       " 'growing',\n",
       " 'grim',\n",
       " 'about',\n",
       " 'the',\n",
       " 'mouth',\n",
       " 'whenever',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'damp',\n",
       " 'drizzly',\n",
       " 'november',\n",
       " 'in',\n",
       " 'my',\n",
       " 'soul',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'find',\n",
       " 'myself',\n",
       " 'involuntarily',\n",
       " 'pausing',\n",
       " 'before',\n",
       " 'coffin',\n",
       " 'warehouses',\n",
       " 'and',\n",
       " 'bringing',\n",
       " 'up',\n",
       " 'the',\n",
       " 'rear',\n",
       " 'of',\n",
       " 'every',\n",
       " 'funeral',\n",
       " 'i',\n",
       " 'meet',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'whenever',\n",
       " 'my',\n",
       " 'hypos',\n",
       " 'get',\n",
       " 'such',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'hand',\n",
       " 'of',\n",
       " 'me',\n",
       " 'that',\n",
       " 'it',\n",
       " 'requires',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'moral',\n",
       " 'principle',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'me',\n",
       " 'from',\n",
       " 'deliberately',\n",
       " 'stepping',\n",
       " 'into',\n",
       " 'the',\n",
       " 'street',\n",
       " 'and',\n",
       " 'methodically',\n",
       " 'knocking',\n",
       " 'people',\n",
       " \"'s\",\n",
       " 'hats',\n",
       " 'off',\n",
       " 'then',\n",
       " 'i',\n",
       " 'account',\n",
       " 'it',\n",
       " 'high',\n",
       " 'time',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'i',\n",
       " 'can',\n",
       " 'this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'substitute',\n",
       " 'for',\n",
       " 'pistol',\n",
       " 'and',\n",
       " 'ball',\n",
       " 'with',\n",
       " 'a',\n",
       " 'philosophical',\n",
       " 'flourish',\n",
       " 'cato',\n",
       " 'throws',\n",
       " 'himself',\n",
       " 'upon',\n",
       " 'his',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'quietly',\n",
       " 'take',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ship',\n",
       " 'there',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'surprising',\n",
       " 'in',\n",
       " 'this',\n",
       " 'if',\n",
       " 'they',\n",
       " 'but',\n",
       " 'knew',\n",
       " 'it',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'men',\n",
       " 'in',\n",
       " 'their',\n",
       " 'degree',\n",
       " 'some',\n",
       " 'time',\n",
       " 'or',\n",
       " 'other',\n",
       " 'cherish',\n",
       " 'very',\n",
       " 'nearly',\n",
       " 'the',\n",
       " 'same',\n",
       " 'feelings',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'ocean',\n",
       " 'with',\n",
       " 'me',\n",
       " 'there',\n",
       " 'now',\n",
       " 'is',\n",
       " 'your',\n",
       " 'insular',\n",
       " 'city',\n",
       " 'of',\n",
       " 'the',\n",
       " 'manhattoes',\n",
       " 'belted',\n",
       " 'round',\n",
       " 'by',\n",
       " 'wharves',\n",
       " 'as',\n",
       " 'indian',\n",
       " 'isles',\n",
       " 'by',\n",
       " 'coral',\n",
       " 'reefs',\n",
       " 'commerce',\n",
       " 'surrounds',\n",
       " 'it',\n",
       " 'with',\n",
       " 'her',\n",
       " 'surf',\n",
       " 'right',\n",
       " 'and',\n",
       " 'left',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'take',\n",
       " 'you',\n",
       " 'waterward',\n",
       " 'its',\n",
       " 'extreme',\n",
       " 'downtown',\n",
       " 'is',\n",
       " 'the',\n",
       " 'battery',\n",
       " 'where',\n",
       " 'that',\n",
       " 'noble',\n",
       " 'mole',\n",
       " 'is',\n",
       " 'washed',\n",
       " 'by',\n",
       " 'waves',\n",
       " 'and',\n",
       " 'cooled',\n",
       " 'by',\n",
       " 'breezes',\n",
       " 'which',\n",
       " 'a',\n",
       " 'few',\n",
       " 'hours',\n",
       " 'previous',\n",
       " 'were',\n",
       " 'out',\n",
       " 'of',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'land',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'crowds',\n",
       " 'of',\n",
       " 'water',\n",
       " 'gazers',\n",
       " 'there',\n",
       " 'circumambulate',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'a',\n",
       " 'dreamy',\n",
       " 'sabbath',\n",
       " 'afternoon',\n",
       " 'go',\n",
       " 'from',\n",
       " 'corlears',\n",
       " 'hook',\n",
       " 'to',\n",
       " 'coenties',\n",
       " 'slip',\n",
       " 'and',\n",
       " 'from',\n",
       " 'thence',\n",
       " 'by',\n",
       " 'whitehall',\n",
       " 'northward',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'see?--posted',\n",
       " 'like',\n",
       " 'silent',\n",
       " 'sentinels',\n",
       " 'all',\n",
       " 'around',\n",
       " 'the',\n",
       " 'town',\n",
       " 'stand',\n",
       " 'thousands',\n",
       " 'upon',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'mortal',\n",
       " 'men',\n",
       " 'fixed',\n",
       " 'in',\n",
       " 'ocean',\n",
       " 'reveries',\n",
       " 'some',\n",
       " 'leaning',\n",
       " 'against',\n",
       " 'the',\n",
       " 'spiles',\n",
       " 'some',\n",
       " 'seated',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'pier',\n",
       " 'heads',\n",
       " 'some',\n",
       " 'looking',\n",
       " 'over',\n",
       " 'the',\n",
       " 'bulwarks',\n",
       " 'of',\n",
       " 'ships',\n",
       " 'from',\n",
       " 'china',\n",
       " 'some',\n",
       " 'high',\n",
       " 'aloft',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rigging',\n",
       " 'as',\n",
       " 'if',\n",
       " 'striving',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'still',\n",
       " 'better',\n",
       " 'seaward',\n",
       " 'peep',\n",
       " 'but',\n",
       " 'these',\n",
       " 'are',\n",
       " 'all',\n",
       " 'landsmen',\n",
       " 'of',\n",
       " 'week',\n",
       " 'days',\n",
       " 'pent',\n",
       " 'up',\n",
       " 'in',\n",
       " 'lath',\n",
       " 'and',\n",
       " 'plaster',\n",
       " 'tied',\n",
       " 'to',\n",
       " 'counters',\n",
       " 'nailed',\n",
       " 'to',\n",
       " 'benches',\n",
       " 'clinched',\n",
       " 'to',\n",
       " 'desks',\n",
       " 'how',\n",
       " 'then',\n",
       " 'is',\n",
       " 'this',\n",
       " 'are',\n",
       " 'the',\n",
       " 'green',\n",
       " 'fields',\n",
       " 'gone',\n",
       " 'what',\n",
       " 'do',\n",
       " 'they',\n",
       " 'here',\n",
       " 'but',\n",
       " 'look',\n",
       " 'here',\n",
       " 'come',\n",
       " 'more',\n",
       " 'crowds',\n",
       " 'pacing',\n",
       " 'straight',\n",
       " 'for',\n",
       " 'the',\n",
       " 'water',\n",
       " 'and',\n",
       " 'seemingly',\n",
       " 'bound',\n",
       " 'for',\n",
       " 'a',\n",
       " 'dive',\n",
       " 'strange',\n",
       " 'nothing',\n",
       " 'will',\n",
       " 'content',\n",
       " 'them',\n",
       " 'but',\n",
       " 'the',\n",
       " 'extremest',\n",
       " 'limit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'land',\n",
       " 'loitering',\n",
       " 'under',\n",
       " 'the',\n",
       " 'shady',\n",
       " 'lee',\n",
       " 'of',\n",
       " 'yonder',\n",
       " 'warehouses',\n",
       " 'will',\n",
       " 'not',\n",
       " 'suffice',\n",
       " 'no',\n",
       " 'they',\n",
       " 'must',\n",
       " 'get',\n",
       " 'just',\n",
       " 'as',\n",
       " 'nigh',\n",
       " 'the',\n",
       " 'water',\n",
       " 'as',\n",
       " 'they',\n",
       " 'possibly',\n",
       " 'can',\n",
       " 'without',\n",
       " 'falling',\n",
       " 'in',\n",
       " 'and',\n",
       " 'there',\n",
       " 'they',\n",
       " 'stand',\n",
       " 'miles',\n",
       " 'of',\n",
       " 'them',\n",
       " 'leagues',\n",
       " 'inlanders',\n",
       " 'all',\n",
       " 'they',\n",
       " 'come',\n",
       " 'from',\n",
       " 'lanes',\n",
       " 'and',\n",
       " 'alleys',\n",
       " 'streets',\n",
       " 'and',\n",
       " 'avenues',\n",
       " 'north',\n",
       " 'east',\n",
       " 'south',\n",
       " 'and',\n",
       " 'west',\n",
       " 'yet',\n",
       " 'here',\n",
       " 'they',\n",
       " 'all',\n",
       " 'unite',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'does',\n",
       " 'the',\n",
       " 'magnetic',\n",
       " 'virtue',\n",
       " 'of',\n",
       " 'the',\n",
       " 'needles',\n",
       " 'of',\n",
       " 'the',\n",
       " 'compasses',\n",
       " 'of',\n",
       " 'all',\n",
       " 'those',\n",
       " 'ships',\n",
       " 'attract',\n",
       " 'them',\n",
       " 'thither',\n",
       " 'once',\n",
       " 'more',\n",
       " 'say',\n",
       " 'you',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'country',\n",
       " 'in',\n",
       " 'some',\n",
       " 'high',\n",
       " 'land',\n",
       " 'of',\n",
       " 'lakes',\n",
       " 'take',\n",
       " 'almost',\n",
       " 'any',\n",
       " 'path',\n",
       " 'you',\n",
       " 'please',\n",
       " 'and',\n",
       " 'ten',\n",
       " 'to',\n",
       " 'one',\n",
       " 'it',\n",
       " 'carries',\n",
       " 'you',\n",
       " 'down',\n",
       " 'in',\n",
       " 'a',\n",
       " 'dale',\n",
       " 'and',\n",
       " 'leaves',\n",
       " 'you',\n",
       " 'there',\n",
       " 'by',\n",
       " 'a',\n",
       " 'pool',\n",
       " 'in',\n",
       " 'the',\n",
       " 'stream',\n",
       " 'there',\n",
       " 'is',\n",
       " 'magic',\n",
       " 'in',\n",
       " 'it',\n",
       " 'let',\n",
       " 'the',\n",
       " 'most',\n",
       " 'absent',\n",
       " 'minded',\n",
       " 'of',\n",
       " 'men',\n",
       " 'be',\n",
       " 'plunged',\n",
       " 'in',\n",
       " 'his',\n",
       " 'deepest',\n",
       " 'reveries',\n",
       " 'stand',\n",
       " 'that',\n",
       " 'man',\n",
       " 'on',\n",
       " 'his',\n",
       " 'legs',\n",
       " 'set',\n",
       " 'his',\n",
       " 'feet',\n",
       " 'a',\n",
       " 'going',\n",
       " 'and',\n",
       " 'he',\n",
       " 'will',\n",
       " 'infallibly',\n",
       " 'lead',\n",
       " 'you',\n",
       " 'to',\n",
       " 'water',\n",
       " 'if',\n",
       " 'water',\n",
       " 'there',\n",
       " 'be',\n",
       " 'in',\n",
       " 'all',\n",
       " 'that',\n",
       " 'region',\n",
       " 'should',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'be',\n",
       " 'athirst',\n",
       " 'in',\n",
       " 'the',\n",
       " 'great',\n",
       " 'american',\n",
       " 'desert',\n",
       " 'try',\n",
       " 'this',\n",
       " 'experiment',\n",
       " 'if',\n",
       " 'your',\n",
       " 'caravan',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'be',\n",
       " 'supplied',\n",
       " 'with',\n",
       " 'a',\n",
       " 'metaphysical',\n",
       " 'professor',\n",
       " 'yes',\n",
       " 'as',\n",
       " 'every',\n",
       " 'one',\n",
       " 'knows',\n",
       " 'meditation',\n",
       " 'and',\n",
       " 'water',\n",
       " 'are',\n",
       " 'wedded',\n",
       " 'for',\n",
       " 'ever',\n",
       " 'but',\n",
       " 'here',\n",
       " 'is',\n",
       " 'an',\n",
       " 'artist',\n",
       " 'he',\n",
       " 'desires',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'you',\n",
       " 'the',\n",
       " 'dreamiest',\n",
       " 'shadiest',\n",
       " 'quietest',\n",
       " 'most',\n",
       " 'enchanting',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'romantic',\n",
       " 'landscape',\n",
       " 'in',\n",
       " 'all',\n",
       " 'the',\n",
       " 'valley',\n",
       " 'of',\n",
       " 'the',\n",
       " 'saco',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'chief',\n",
       " 'element',\n",
       " 'he',\n",
       " 'employs',\n",
       " 'there',\n",
       " 'stand',\n",
       " 'his',\n",
       " 'trees',\n",
       " 'each',\n",
       " 'with',\n",
       " 'a',\n",
       " 'hollow',\n",
       " 'trunk',\n",
       " 'as',\n",
       " 'if',\n",
       " 'a',\n",
       " 'hermit',\n",
       " 'and',\n",
       " 'a',\n",
       " 'crucifix',\n",
       " 'were',\n",
       " 'within',\n",
       " 'and',\n",
       " 'here',\n",
       " 'sleeps',\n",
       " 'his',\n",
       " 'meadow',\n",
       " 'and',\n",
       " 'there',\n",
       " 'sleep',\n",
       " 'his',\n",
       " 'cattle',\n",
       " 'and',\n",
       " 'up',\n",
       " 'from',\n",
       " 'yonder',\n",
       " 'cottage',\n",
       " 'goes',\n",
       " 'a',\n",
       " 'sleepy',\n",
       " 'smoke',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'distant',\n",
       " 'woodlands',\n",
       " 'winds',\n",
       " 'a',\n",
       " 'mazy',\n",
       " 'way',\n",
       " 'reaching',\n",
       " 'to',\n",
       " 'overlapping',\n",
       " 'spurs',\n",
       " 'of',\n",
       " 'mountains',\n",
       " 'bathed',\n",
       " 'in',\n",
       " 'their',\n",
       " 'hill',\n",
       " 'side',\n",
       " 'blue',\n",
       " 'but',\n",
       " 'though',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'lies',\n",
       " 'thus',\n",
       " 'tranced',\n",
       " 'and',\n",
       " 'though',\n",
       " 'this',\n",
       " 'pine',\n",
       " 'tree',\n",
       " 'shakes',\n",
       " 'down',\n",
       " 'its',\n",
       " 'sighs',\n",
       " 'like',\n",
       " 'leaves',\n",
       " 'upon',\n",
       " 'this',\n",
       " 'shepherd',\n",
       " \"'s\",\n",
       " 'head',\n",
       " 'yet',\n",
       " 'all',\n",
       " 'were',\n",
       " 'vain',\n",
       " 'unless',\n",
       " 'the',\n",
       " 'shepherd',\n",
       " \"'s\",\n",
       " 'eye',\n",
       " 'were',\n",
       " 'fixed',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'stream',\n",
       " 'before',\n",
       " 'him',\n",
       " 'go',\n",
       " 'visit',\n",
       " 'the',\n",
       " 'prairies',\n",
       " 'in',\n",
       " 'june',\n",
       " 'when',\n",
       " 'for',\n",
       " 'scores',\n",
       " 'on',\n",
       " 'scores',\n",
       " 'of',\n",
       " 'miles',\n",
       " 'you',\n",
       " 'wade',\n",
       " 'knee',\n",
       " 'deep',\n",
       " 'among',\n",
       " 'tiger',\n",
       " 'lilies',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'one',\n",
       " 'charm',\n",
       " 'wanting?--water',\n",
       " 'there',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'drop',\n",
       " 'of',\n",
       " 'water',\n",
       " 'there',\n",
       " 'were',\n",
       " 'niagara',\n",
       " 'but',\n",
       " 'a',\n",
       " 'cataract',\n",
       " 'of',\n",
       " 'sand',\n",
       " 'would',\n",
       " 'you',\n",
       " 'travel',\n",
       " 'your',\n",
       " 'thousand',\n",
       " 'miles',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'poor',\n",
       " 'poet',\n",
       " 'of',\n",
       " 'tennessee',\n",
       " 'upon',\n",
       " 'suddenly',\n",
       " 'receiving',\n",
       " 'two',\n",
       " 'handfuls',\n",
       " 'of',\n",
       " 'silver',\n",
       " 'deliberate',\n",
       " 'whether',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'him',\n",
       " 'a',\n",
       " 'coat',\n",
       " 'which',\n",
       " 'he',\n",
       " 'sadly',\n",
       " 'needed',\n",
       " 'or',\n",
       " 'invest',\n",
       " 'his',\n",
       " 'money',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pedestrian',\n",
       " 'trip',\n",
       " 'to',\n",
       " 'rockaway',\n",
       " 'beach',\n",
       " 'why',\n",
       " 'is',\n",
       " 'almost',\n",
       " 'every',\n",
       " 'robust',\n",
       " 'healthy',\n",
       " 'boy',\n",
       " 'with',\n",
       " 'a',\n",
       " 'robust',\n",
       " 'healthy',\n",
       " 'soul',\n",
       " 'in',\n",
       " 'him',\n",
       " 'at',\n",
       " 'some',\n",
       " 'time',\n",
       " 'or',\n",
       " 'other',\n",
       " 'crazy',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'why',\n",
       " 'upon',\n",
       " 'your',\n",
       " 'first',\n",
       " 'voyage',\n",
       " 'as',\n",
       " 'a',\n",
       " 'passenger',\n",
       " 'did',\n",
       " 'you',\n",
       " 'yourself',\n",
       " 'feel',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mystical',\n",
       " 'vibration',\n",
       " 'when',\n",
       " 'first',\n",
       " 'told',\n",
       " 'that',\n",
       " 'you',\n",
       " 'and',\n",
       " 'your',\n",
       " 'ship',\n",
       " 'were',\n",
       " 'now',\n",
       " 'out',\n",
       " 'of',\n",
       " 'sight',\n",
       " 'of',\n",
       " 'land',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'old',\n",
       " 'persians',\n",
       " 'hold',\n",
       " 'the',\n",
       " 'sea',\n",
       " 'holy',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'greeks',\n",
       " 'give',\n",
       " 'it',\n",
       " 'a',\n",
       " 'separate',\n",
       " 'deity',\n",
       " 'and',\n",
       " 'own',\n",
       " 'brother',\n",
       " 'of',\n",
       " 'jove',\n",
       " 'surely',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'without',\n",
       " 'meaning',\n",
       " 'and',\n",
       " 'still',\n",
       " 'deeper',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'that',\n",
       " 'story',\n",
       " 'of',\n",
       " 'narcissus',\n",
       " 'who',\n",
       " 'because',\n",
       " 'he',\n",
       " 'could',\n",
       " 'not',\n",
       " 'grasp',\n",
       " 'the',\n",
       " 'tormenting',\n",
       " 'mild',\n",
       " 'image',\n",
       " 'he',\n",
       " 'saw',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fountain',\n",
       " 'plunged',\n",
       " 'into',\n",
       " 'it',\n",
       " 'and',\n",
       " 'was',\n",
       " 'drowned',\n",
       " 'but',\n",
       " 'that',\n",
       " 'same',\n",
       " 'image',\n",
       " 'we',\n",
       " 'ourselves',\n",
       " 'see',\n",
       " 'in',\n",
       " 'all',\n",
       " 'rivers',\n",
       " 'and',\n",
       " 'oceans',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'image',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ungraspable',\n",
       " 'phantom',\n",
       " 'of',\n",
       " 'life',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'key',\n",
       " 'to',\n",
       " 'it',\n",
       " 'all',\n",
       " 'now',\n",
       " 'when',\n",
       " 'i',\n",
       " 'say',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'the',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'going',\n",
       " 'to',\n",
       " 'sea',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'hazy',\n",
       " 'about',\n",
       " 'the',\n",
       " 'eyes',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 50 training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "# Bu kod her bir 26'lık paketi yani tokens[0:26], tokens[1:27], tokens[2:28] text_sequences'in içine ayrı ayrı listeler olarak topluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer ile Sayıya Dönüştürmek (Sıralı Sayılar ile Temsil Etmek):\n",
    "\n",
    "- Bu yöntem, her bir kelimeyi bir token'a, yani bir sayıya dönüştürür. Bu sayı genellikle kelimenin metindeki sırasına veya bir kelime sözlüğüne dayalı bir indeksleme işlemine göre atanır. Bu yöntemde kelimelerin sırası korunur ancak kelimenin anlamı göz ardı edilir.\n",
    "\n",
    "Farkı: \n",
    "\n",
    "- Tokenizer, kelimeleri basitçe bir dizi sayıya dönüştürür. Kelimenin anlamına veya bağlama dayalı bir vektörel ilişki kurulmaz. Her kelime bir sayı ile temsil edilir (örneğin, \"kedi\" = 15, \"köpek\" = 23 gibi).\n",
    "\n",
    "Avantajları:\n",
    "\n",
    "- Daha basit ve hızlıdır: Kelimeler sadece sayılarla temsil edildiği için işlem hızı ve maliyeti daha düşüktür.\n",
    "- Küçük ve sabit boyutlu bir vektör üretir: Örneğin, her kelime bir sayıya dönüştürüldüğünde kelimeler sabit uzunluktaki bir dizide yer alır, bu da hesaplamayı kolaylaştırır.\n",
    "\n",
    "Dezavantajları:\n",
    "\n",
    "- Anlamsal bilgi kaybolur: Aynı anlama sahip iki kelime (örneğin \"mutlu\" ve \"sevinçli\") tamamen farklı sayılarla temsil edilebilir, bu da modelin anlamsal ilişkiyi öğrenmesini zorlaştırır.\n",
    "- Bağlamı dikkate almaz: Her kelime kendi başına ele alınır ve diğer kelimelerle olan ilişkisi göz ardı edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)    # Her bir kelimeyi sayıya dönüştürüyor. Ama bu aşamada sözlük oluşturuyor gibi düşün. Cümleleri sayıya dönüştürmüyor.\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)  # Her cümledeki kelimeleri sayılara dönüştürüyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[956,\n",
       " 14,\n",
       " 263,\n",
       " 51,\n",
       " 261,\n",
       " 408,\n",
       " 87,\n",
       " 219,\n",
       " 129,\n",
       " 111,\n",
       " 954,\n",
       " 260,\n",
       " 50,\n",
       " 43,\n",
       " 38,\n",
       " 314,\n",
       " 7,\n",
       " 23,\n",
       " 546,\n",
       " 3,\n",
       " 150,\n",
       " 259,\n",
       " 6,\n",
       " 2713,\n",
       " 14,\n",
       " 24]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 263,\n",
       " 51,\n",
       " 261,\n",
       " 408,\n",
       " 87,\n",
       " 219,\n",
       " 129,\n",
       " 111,\n",
       " 954,\n",
       " 260,\n",
       " 50,\n",
       " 43,\n",
       " 38,\n",
       " 314,\n",
       " 7,\n",
       " 23,\n",
       " 546,\n",
       " 3,\n",
       " 150,\n",
       " 259,\n",
       " 6,\n",
       " 2713,\n",
       " 14,\n",
       " 24,\n",
       " 957]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'a',\n",
       " 3: 'and',\n",
       " 4: 'of',\n",
       " 5: 'i',\n",
       " 6: 'to',\n",
       " 7: 'in',\n",
       " 8: 'it',\n",
       " 9: 'that',\n",
       " 10: 'he',\n",
       " 11: 'his',\n",
       " 12: 'was',\n",
       " 13: 'but',\n",
       " 14: 'me',\n",
       " 15: 'with',\n",
       " 16: 'as',\n",
       " 17: 'at',\n",
       " 18: 'this',\n",
       " 19: 'you',\n",
       " 20: 'is',\n",
       " 21: 'all',\n",
       " 22: 'for',\n",
       " 23: 'my',\n",
       " 24: 'on',\n",
       " 25: 'be',\n",
       " 26: \"'s\",\n",
       " 27: 'not',\n",
       " 28: 'from',\n",
       " 29: 'there',\n",
       " 30: 'one',\n",
       " 31: 'up',\n",
       " 32: 'what',\n",
       " 33: 'him',\n",
       " 34: 'so',\n",
       " 35: 'bed',\n",
       " 36: 'now',\n",
       " 37: 'about',\n",
       " 38: 'no',\n",
       " 39: 'into',\n",
       " 40: 'by',\n",
       " 41: 'were',\n",
       " 42: 'out',\n",
       " 43: 'or',\n",
       " 44: 'harpooneer',\n",
       " 45: 'had',\n",
       " 46: 'then',\n",
       " 47: 'have',\n",
       " 48: 'an',\n",
       " 49: 'upon',\n",
       " 50: 'little',\n",
       " 51: 'some',\n",
       " 52: 'old',\n",
       " 53: 'like',\n",
       " 54: 'if',\n",
       " 55: 'they',\n",
       " 56: 'would',\n",
       " 57: 'do',\n",
       " 58: 'over',\n",
       " 59: 'landlord',\n",
       " 60: 'thought',\n",
       " 61: 'room',\n",
       " 62: 'when',\n",
       " 63: 'could',\n",
       " 64: \"n't\",\n",
       " 65: 'night',\n",
       " 66: 'here',\n",
       " 67: 'head',\n",
       " 68: 'such',\n",
       " 69: 'which',\n",
       " 70: 'man',\n",
       " 71: 'did',\n",
       " 72: 'sea',\n",
       " 73: 'time',\n",
       " 74: 'other',\n",
       " 75: 'very',\n",
       " 76: 'go',\n",
       " 77: 'these',\n",
       " 78: 'more',\n",
       " 79: 'though',\n",
       " 80: 'first',\n",
       " 81: 'sort',\n",
       " 82: 'said',\n",
       " 83: 'last',\n",
       " 84: 'down',\n",
       " 85: 'most',\n",
       " 86: 'been',\n",
       " 87: 'never',\n",
       " 88: 'your',\n",
       " 89: 'them',\n",
       " 90: 'must',\n",
       " 91: 'tell',\n",
       " 92: 'much',\n",
       " 93: 'good',\n",
       " 94: 'see',\n",
       " 95: 'off',\n",
       " 96: 'myself',\n",
       " 97: 'are',\n",
       " 98: 'yet',\n",
       " 99: 'sleep',\n",
       " 100: 'who',\n",
       " 101: 'seemed',\n",
       " 102: 'light',\n",
       " 103: 'way',\n",
       " 104: 'their',\n",
       " 105: 'just',\n",
       " 106: 'being',\n",
       " 107: 'than',\n",
       " 108: 'place',\n",
       " 109: 'queequeg',\n",
       " 110: 'great',\n",
       " 111: 'long',\n",
       " 112: 'before',\n",
       " 113: 'get',\n",
       " 114: 'round',\n",
       " 115: 'where',\n",
       " 116: 'still',\n",
       " 117: 'any',\n",
       " 118: 'too',\n",
       " 119: 'only',\n",
       " 120: 'door',\n",
       " 121: 'can',\n",
       " 122: 'himself',\n",
       " 123: 'heads',\n",
       " 124: 'come',\n",
       " 125: 'ever',\n",
       " 126: 'two',\n",
       " 127: 'enough',\n",
       " 128: 'made',\n",
       " 129: 'how',\n",
       " 130: 'hand',\n",
       " 131: 'same',\n",
       " 132: 'looking',\n",
       " 133: 'something',\n",
       " 134: 'may',\n",
       " 135: \"'\",\n",
       " 136: 'almost',\n",
       " 137: 'say',\n",
       " 138: 'should',\n",
       " 139: 'side',\n",
       " 140: 'why',\n",
       " 141: 'own',\n",
       " 142: 'we',\n",
       " 143: 'new',\n",
       " 144: 'again',\n",
       " 145: 'came',\n",
       " 146: 'arm',\n",
       " 147: 'house',\n",
       " 148: 'away',\n",
       " 149: 'might',\n",
       " 150: 'nothing',\n",
       " 151: 'take',\n",
       " 152: 'towards',\n",
       " 153: 'will',\n",
       " 154: 'under',\n",
       " 155: 'going',\n",
       " 156: 'make',\n",
       " 157: 'whale',\n",
       " 158: 'stood',\n",
       " 159: 'boots',\n",
       " 160: 'ye',\n",
       " 161: 'back',\n",
       " 162: \"'ll\",\n",
       " 163: 'tomahawk',\n",
       " 164: 'part',\n",
       " 165: 'world',\n",
       " 166: 'soon',\n",
       " 167: 'water',\n",
       " 168: 'against',\n",
       " 169: 'those',\n",
       " 170: 'between',\n",
       " 171: 'after',\n",
       " 172: 'whaling',\n",
       " 173: 'lay',\n",
       " 174: 'took',\n",
       " 175: 'half',\n",
       " 176: 'began',\n",
       " 177: 'face',\n",
       " 178: 'streets',\n",
       " 179: 'land',\n",
       " 180: 'better',\n",
       " 181: 'once',\n",
       " 182: 'voyage',\n",
       " 183: 'give',\n",
       " 184: 'rather',\n",
       " 185: 'well',\n",
       " 186: 'however',\n",
       " 187: 'else',\n",
       " 188: 'heard',\n",
       " 189: 'put',\n",
       " 190: 'stop',\n",
       " 191: 'dark',\n",
       " 192: 'went',\n",
       " 193: 'black',\n",
       " 194: 'window',\n",
       " 195: 'cannibal',\n",
       " 196: 'fire',\n",
       " 197: 'every',\n",
       " 198: 'ship',\n",
       " 199: 'stand',\n",
       " 200: 'strange',\n",
       " 201: 'without',\n",
       " 202: 'feet',\n",
       " 203: 'whether',\n",
       " 204: 'because',\n",
       " 205: 'eyes',\n",
       " 206: 'think',\n",
       " 207: 'thinks',\n",
       " 208: 'idea',\n",
       " 209: 'bag',\n",
       " 210: 'nantucket',\n",
       " 211: 'late',\n",
       " 212: 'cold',\n",
       " 213: 'our',\n",
       " 214: 'found',\n",
       " 215: 'full',\n",
       " 216: 'morning',\n",
       " 217: 'sleeping',\n",
       " 218: 'got',\n",
       " 219: 'mind',\n",
       " 220: 'her',\n",
       " 221: 'right',\n",
       " 222: 'its',\n",
       " 223: 'look',\n",
       " 224: 'town',\n",
       " 225: 'south',\n",
       " 226: 'does',\n",
       " 227: 'let',\n",
       " 228: 'set',\n",
       " 229: 'yourself',\n",
       " 230: 'image',\n",
       " 231: 'saw',\n",
       " 232: 'am',\n",
       " 233: 'besides',\n",
       " 234: 'sailor',\n",
       " 235: 'seas',\n",
       " 236: 'rolled',\n",
       " 237: 'till',\n",
       " 238: 'day',\n",
       " 239: 'sign',\n",
       " 240: 'looked',\n",
       " 241: 'hard',\n",
       " 242: 'moment',\n",
       " 243: 'corner',\n",
       " 244: 'entry',\n",
       " 245: 'four',\n",
       " 246: 'wall',\n",
       " 247: 'savage',\n",
       " 248: 'table',\n",
       " 249: 'indeed',\n",
       " 250: 'bench',\n",
       " 251: 'chest',\n",
       " 252: 'while',\n",
       " 253: 'stranger',\n",
       " 254: 'possible',\n",
       " 255: 'feeling',\n",
       " 256: 'floor',\n",
       " 257: 'squares',\n",
       " 258: 'hat',\n",
       " 259: 'particular',\n",
       " 260: 'having',\n",
       " 261: 'years',\n",
       " 262: 'harpoon',\n",
       " 263: 'ishmael',\n",
       " 264: 'whenever',\n",
       " 265: 'mouth',\n",
       " 266: 'high',\n",
       " 267: 'knew',\n",
       " 268: 'men',\n",
       " 269: 'hours',\n",
       " 270: 'green',\n",
       " 271: 'bit',\n",
       " 272: 'within',\n",
       " 273: 'picture',\n",
       " 274: 'told',\n",
       " 275: 'story',\n",
       " 276: 'mean',\n",
       " 277: 'speak',\n",
       " 278: 'order',\n",
       " 279: 'making',\n",
       " 280: 'even',\n",
       " 281: 'perhaps',\n",
       " 282: 'things',\n",
       " 283: 'answer',\n",
       " 284: 'parts',\n",
       " 285: 'wild',\n",
       " 286: 'reason',\n",
       " 287: 'young',\n",
       " 288: 'craft',\n",
       " 289: 'business',\n",
       " 290: 'dead',\n",
       " 291: 'another',\n",
       " 292: 'middle',\n",
       " 293: 'sure',\n",
       " 294: 'candle',\n",
       " 295: 'presently',\n",
       " 296: 'low',\n",
       " 297: 'turned',\n",
       " 298: 'teeth',\n",
       " 299: 'dim',\n",
       " 300: 'euroclydon',\n",
       " 301: 'kept',\n",
       " 302: 'glass',\n",
       " 303: 'afterwards',\n",
       " 304: 'large',\n",
       " 305: 'three',\n",
       " 306: 'telling',\n",
       " 307: 'getting',\n",
       " 308: 'small',\n",
       " 309: 'next',\n",
       " 310: 'seeing',\n",
       " 311: 'sell',\n",
       " 312: 'felt',\n",
       " 313: 'sun',\n",
       " 314: 'money',\n",
       " 315: 'sail',\n",
       " 316: 'coffin',\n",
       " 317: 'especially',\n",
       " 318: 'street',\n",
       " 319: 'city',\n",
       " 320: 'few',\n",
       " 321: 'previous',\n",
       " 322: 'sight',\n",
       " 323: 'days',\n",
       " 324: 'straight',\n",
       " 325: 'nigh',\n",
       " 326: 'legs',\n",
       " 327: 'try',\n",
       " 328: 'yes',\n",
       " 329: 'unless',\n",
       " 330: 'poor',\n",
       " 331: 'coat',\n",
       " 332: 'passenger',\n",
       " 333: 'taking',\n",
       " 334: 'true',\n",
       " 335: 'thing',\n",
       " 336: 'ai',\n",
       " 337: 'always',\n",
       " 338: 'us',\n",
       " 339: 'really',\n",
       " 340: 'marvellous',\n",
       " 341: 'heaven',\n",
       " 342: 'air',\n",
       " 343: 'far',\n",
       " 344: 'second',\n",
       " 345: 'many',\n",
       " 346: 'has',\n",
       " 347: 'unaccountable',\n",
       " 348: 'grand',\n",
       " 349: 'jolly',\n",
       " 350: 'open',\n",
       " 351: 'shirt',\n",
       " 352: 'cape',\n",
       " 353: 'bedford',\n",
       " 354: 'fine',\n",
       " 355: 'further',\n",
       " 356: 'ice',\n",
       " 357: 'frost',\n",
       " 358: 'foot',\n",
       " 359: 'wide',\n",
       " 360: 'white',\n",
       " 361: 'tall',\n",
       " 362: 'i.',\n",
       " 363: 'wooden',\n",
       " 364: 'worse',\n",
       " 365: 'death',\n",
       " 366: 'mine',\n",
       " 367: 'lazarus',\n",
       " 368: 'keep',\n",
       " 369: 'along',\n",
       " 370: 'hung',\n",
       " 371: 'throwing',\n",
       " 372: 'centre',\n",
       " 373: 'rest',\n",
       " 374: 'fact',\n",
       " 375: 'hair',\n",
       " 376: 'broken',\n",
       " 377: 'kill',\n",
       " 378: 'through',\n",
       " 379: 'chimney',\n",
       " 380: 'fancy',\n",
       " 381: 'bar',\n",
       " 382: 'trying',\n",
       " 383: 'dumplings',\n",
       " 384: 'heavens',\n",
       " 385: 'manner',\n",
       " 386: 'devil',\n",
       " 387: 'together',\n",
       " 388: 'seen',\n",
       " 389: 'deal',\n",
       " 390: 'know',\n",
       " 391: 'skin',\n",
       " 392: 'ca',\n",
       " 393: 'shavings',\n",
       " 394: 'peddling',\n",
       " 395: 'sunday',\n",
       " 396: 'counterpane',\n",
       " 397: 'mat',\n",
       " 398: 'christian',\n",
       " 399: 'commenced',\n",
       " 400: 'thinking',\n",
       " 401: 'similar',\n",
       " 402: 'afraid',\n",
       " 403: 'length',\n",
       " 404: 'idol',\n",
       " 405: 'e',\n",
       " 406: 'sabbee',\n",
       " 407: 'waking',\n",
       " 408: 'ago',\n",
       " 409: 'find',\n",
       " 410: 'damp',\n",
       " 411: 'soul',\n",
       " 412: 'strong',\n",
       " 413: 'account',\n",
       " 414: 'sword',\n",
       " 415: 'quietly',\n",
       " 416: 'degree',\n",
       " 417: 'left',\n",
       " 418: 'around',\n",
       " 419: 'fixed',\n",
       " 420: 'ships',\n",
       " 421: 'miles',\n",
       " 422: 'country',\n",
       " 423: 'stream',\n",
       " 424: 'lead',\n",
       " 425: 'american',\n",
       " 426: 'artist',\n",
       " 427: 'each',\n",
       " 428: 'goes',\n",
       " 429: 'deep',\n",
       " 430: 'distant',\n",
       " 431: 'winds',\n",
       " 432: 'blue',\n",
       " 433: 'among',\n",
       " 434: 'suddenly',\n",
       " 435: 'feel',\n",
       " 436: 'meaning',\n",
       " 437: 'phantom',\n",
       " 438: 'life',\n",
       " 439: 'passengers',\n",
       " 440: 'nor',\n",
       " 441: 'kind',\n",
       " 442: 'quite',\n",
       " 443: 'care',\n",
       " 444: 'board',\n",
       " 445: 'somehow',\n",
       " 446: 'broiled',\n",
       " 447: 'mast',\n",
       " 448: 'sense',\n",
       " 449: 'knowing',\n",
       " 450: 'either',\n",
       " 451: 'passed',\n",
       " 452: 'hands',\n",
       " 453: 'paying',\n",
       " 454: 'pay',\n",
       " 455: 'penny',\n",
       " 456: 'sailors',\n",
       " 457: 'exactly',\n",
       " 458: 'short',\n",
       " 459: 'easy',\n",
       " 460: 'portentous',\n",
       " 461: 'island',\n",
       " 462: 'nameless',\n",
       " 463: 'sounds',\n",
       " 464: 'since',\n",
       " 465: 'snow',\n",
       " 466: 'saturday',\n",
       " 467: 'matter',\n",
       " 468: 'red',\n",
       " 469: 'partly',\n",
       " 470: 'ere',\n",
       " 471: 'became',\n",
       " 472: 'meanwhile',\n",
       " 473: 'pocket',\n",
       " 474: 'darkness',\n",
       " 475: 'fish',\n",
       " 476: 'inn',\n",
       " 477: 'watch',\n",
       " 478: 'broad',\n",
       " 479: 'entering',\n",
       " 480: 'ha',\n",
       " 481: 'ashes',\n",
       " 482: 'opened',\n",
       " 483: 'spouter',\n",
       " 484: 'name',\n",
       " 485: 'suppose',\n",
       " 486: 'quiet',\n",
       " 487: 'best',\n",
       " 488: 'tempestuous',\n",
       " 489: 'says',\n",
       " 490: 'thou',\n",
       " 491: 'both',\n",
       " 492: 'occurred',\n",
       " 493: 'dives',\n",
       " 494: 'holding',\n",
       " 495: 'frozen',\n",
       " 496: 'altogether',\n",
       " 497: 'plain',\n",
       " 498: 'whom',\n",
       " 499: 'clean',\n",
       " 500: 'human',\n",
       " 501: 'entered',\n",
       " 502: 'wrinkled',\n",
       " 503: 'shelf',\n",
       " 504: 'jonah',\n",
       " 505: 'blanket',\n",
       " 506: \"goin'\",\n",
       " 507: 'bitter',\n",
       " 508: 'supper',\n",
       " 509: 'sat',\n",
       " 510: 'settle',\n",
       " 511: 'chap',\n",
       " 512: 'help',\n",
       " 513: 'spend',\n",
       " 514: 'landed',\n",
       " 515: 'standing',\n",
       " 516: 'held',\n",
       " 517: 'somewhat',\n",
       " 518: 'sober',\n",
       " 519: 'whole',\n",
       " 520: 'dam',\n",
       " 521: 'brown',\n",
       " 522: 'bulkington',\n",
       " 523: \"o'clock\",\n",
       " 524: 'none',\n",
       " 525: 'coming',\n",
       " 526: \"'ve\",\n",
       " 527: 'wait',\n",
       " 528: 'plane',\n",
       " 529: 'saying',\n",
       " 530: 'grinning',\n",
       " 531: 'placed',\n",
       " 532: 'shouted',\n",
       " 533: 'bedfellow',\n",
       " 534: 'zealand',\n",
       " 535: 'sal',\n",
       " 536: 'wash',\n",
       " 537: 'thrown',\n",
       " 538: 'purplish',\n",
       " 539: 'turn',\n",
       " 540: 'completely',\n",
       " 541: 'fear',\n",
       " 542: 'grego',\n",
       " 543: 'baby',\n",
       " 544: 'slowly',\n",
       " 545: 'civilized',\n",
       " 546: 'purse',\n",
       " 547: 'monkey',\n",
       " 548: 'involuntarily',\n",
       " 549: 'pausing',\n",
       " 550: 'warehouses',\n",
       " 551: 'requires',\n",
       " 552: 'people',\n",
       " 553: 'nearly',\n",
       " 554: 'ocean',\n",
       " 555: 'indian',\n",
       " 556: 'waterward',\n",
       " 557: 'battery',\n",
       " 558: 'noble',\n",
       " 559: 'washed',\n",
       " 560: 'crowds',\n",
       " 561: 'sabbath',\n",
       " 562: 'afternoon',\n",
       " 563: 'thence',\n",
       " 564: 'silent',\n",
       " 565: 'thousands',\n",
       " 566: 'reveries',\n",
       " 567: 'leaning',\n",
       " 568: 'seated',\n",
       " 569: 'bulwarks',\n",
       " 570: 'aloft',\n",
       " 571: 'week',\n",
       " 572: 'plaster',\n",
       " 573: 'gone',\n",
       " 574: 'bound',\n",
       " 575: 'content',\n",
       " 576: 'yonder',\n",
       " 577: 'falling',\n",
       " 578: 'north',\n",
       " 579: 'please',\n",
       " 580: 'ten',\n",
       " 581: 'leaves',\n",
       " 582: 'magic',\n",
       " 583: 'plunged',\n",
       " 584: 'metaphysical',\n",
       " 585: 'chief',\n",
       " 586: 'trunk',\n",
       " 587: 'meadow',\n",
       " 588: 'smoke',\n",
       " 589: 'reaching',\n",
       " 590: 'hill',\n",
       " 591: 'thus',\n",
       " 592: 'pine',\n",
       " 593: 'sighs',\n",
       " 594: 'shepherd',\n",
       " 595: 'june',\n",
       " 596: 'scores',\n",
       " 597: 'thousand',\n",
       " 598: 'sadly',\n",
       " 599: 'robust',\n",
       " 600: 'healthy',\n",
       " 601: 'boy',\n",
       " 602: 'crazy',\n",
       " 603: 'hold',\n",
       " 604: 'holy',\n",
       " 605: 'brother',\n",
       " 606: 'ourselves',\n",
       " 607: 'begin',\n",
       " 608: 'grow',\n",
       " 609: 'inferred',\n",
       " 610: 'needs',\n",
       " 611: \"don't\",\n",
       " 612: 'themselves',\n",
       " 613: 'commodore',\n",
       " 614: 'captain',\n",
       " 615: 'cook',\n",
       " 616: 'glory',\n",
       " 617: 'whatsoever',\n",
       " 618: 'confess',\n",
       " 619: 'officer',\n",
       " 620: 'respectfully',\n",
       " 621: 'horse',\n",
       " 622: 'huge',\n",
       " 623: 'houses',\n",
       " 624: 'forecastle',\n",
       " 625: 'jump',\n",
       " 626: 'spar',\n",
       " 627: 'particularly',\n",
       " 628: 'putting',\n",
       " 629: 'tar',\n",
       " 630: 'schoolmaster',\n",
       " 631: 'boys',\n",
       " 632: 'transition',\n",
       " 633: 'grin',\n",
       " 634: 'bear',\n",
       " 635: 'hunks',\n",
       " 636: 'sweep',\n",
       " 637: 'weighed',\n",
       " 638: 'anything',\n",
       " 639: 'less',\n",
       " 640: 'thump',\n",
       " 641: 'point',\n",
       " 642: 'view',\n",
       " 643: 'single',\n",
       " 644: 'difference',\n",
       " 645: 'act',\n",
       " 646: 'uncomfortable',\n",
       " 647: 'compare',\n",
       " 648: 'earthly',\n",
       " 649: 'enter',\n",
       " 650: 'deck',\n",
       " 651: 'quarter',\n",
       " 652: 'leaders',\n",
       " 653: 'smelt',\n",
       " 654: 'fates',\n",
       " 655: 'doubtless',\n",
       " 656: 'formed',\n",
       " 657: 'run',\n",
       " 658: 'stage',\n",
       " 659: 'shabby',\n",
       " 660: 'others',\n",
       " 661: 'circumstances',\n",
       " 662: 'motives',\n",
       " 663: 'various',\n",
       " 664: 'mysterious',\n",
       " 665: 'curiosity',\n",
       " 666: 'tormented',\n",
       " 667: 'everlasting',\n",
       " 668: 'wonder',\n",
       " 669: 'purpose',\n",
       " 670: 'floated',\n",
       " 671: 'stuffed',\n",
       " 672: 'horn',\n",
       " 673: 'arrived',\n",
       " 674: 'offer',\n",
       " 675: 'following',\n",
       " 676: 'embark',\n",
       " 677: 'pleased',\n",
       " 678: 'original',\n",
       " 679: 'stranded',\n",
       " 680: 'leviathan',\n",
       " 681: 'whales',\n",
       " 682: 'nay',\n",
       " 683: 'dismal',\n",
       " 684: 'wherever',\n",
       " 685: 'dreary',\n",
       " 686: 'crossed',\n",
       " 687: 'expensive',\n",
       " 688: 'bright',\n",
       " 689: 'windows',\n",
       " 690: 'packed',\n",
       " 691: 'inches',\n",
       " 692: 'thick',\n",
       " 693: 'hear',\n",
       " 694: 'tinkling',\n",
       " 695: 'glasses',\n",
       " 696: 'blackness',\n",
       " 697: 'moving',\n",
       " 698: 'hour',\n",
       " 699: 'proved',\n",
       " 700: 'meant',\n",
       " 701: 'public',\n",
       " 702: 'box',\n",
       " 703: 'flying',\n",
       " 704: 'harpoons',\n",
       " 705: 'trap',\n",
       " 706: 'loud',\n",
       " 707: 'voice',\n",
       " 708: 'sitting',\n",
       " 709: 'beyond',\n",
       " 710: 'negro',\n",
       " 711: 'creaking',\n",
       " 712: 'swinging',\n",
       " 713: 'painting',\n",
       " 714: 'representing',\n",
       " 715: 'connexion',\n",
       " 716: 'peter',\n",
       " 717: 'itself',\n",
       " 718: 'carted',\n",
       " 719: 'burnt',\n",
       " 720: 'spot',\n",
       " 721: 'queer',\n",
       " 722: 'gable',\n",
       " 723: 'ended',\n",
       " 724: 'sharp',\n",
       " 725: 'wind',\n",
       " 726: 'howling',\n",
       " 727: 'nevertheless',\n",
       " 728: 'called',\n",
       " 729: 'outside',\n",
       " 730: 'passage',\n",
       " 731: 'body',\n",
       " 732: 'curbstone',\n",
       " 733: 'corn',\n",
       " 734: 'pooh',\n",
       " 735: 'northern',\n",
       " 736: 'lights',\n",
       " 737: 'summer',\n",
       " 738: 'lengthwise',\n",
       " 739: 'gods',\n",
       " 740: 'lie',\n",
       " 741: 'plenty',\n",
       " 742: 'study',\n",
       " 743: 'series',\n",
       " 744: 'arrive',\n",
       " 745: 'shadows',\n",
       " 746: 'dint',\n",
       " 747: 'conclusion',\n",
       " 748: 'confounded',\n",
       " 749: 'limber',\n",
       " 750: 'truly',\n",
       " 751: 'drive',\n",
       " 752: 'unimaginable',\n",
       " 753: 'midnight',\n",
       " 754: 'unnatural',\n",
       " 755: 'breaking',\n",
       " 756: 'design',\n",
       " 757: 'opposite',\n",
       " 758: 'monstrous',\n",
       " 759: 'knots',\n",
       " 760: 'vast',\n",
       " 761: 'handle',\n",
       " 762: 'wondered',\n",
       " 763: 'mixed',\n",
       " 764: 'deformed',\n",
       " 765: 'flung',\n",
       " 766: 'iron',\n",
       " 767: 'arched',\n",
       " 768: 'cut',\n",
       " 769: 'times',\n",
       " 770: 'beneath',\n",
       " 771: 'covered',\n",
       " 772: 'gathered',\n",
       " 773: 'stands',\n",
       " 774: 'rude',\n",
       " 775: 'bone',\n",
       " 776: 'abominable',\n",
       " 777: 'measure',\n",
       " 778: 'seamen',\n",
       " 779: 'skrimshander',\n",
       " 780: 'sought',\n",
       " 781: 'added',\n",
       " 782: 'forehead',\n",
       " 783: 'objections',\n",
       " 784: \"'d\",\n",
       " 785: 'used',\n",
       " 786: 'depend',\n",
       " 787: 'decent',\n",
       " 788: 'want',\n",
       " 789: 'ready',\n",
       " 790: 'working',\n",
       " 791: 'space',\n",
       " 792: 'lips',\n",
       " 793: 'fingers',\n",
       " 794: 'fare',\n",
       " 795: 'fellow',\n",
       " 796: 'nightmare',\n",
       " 797: 'nt',\n",
       " 798: 'complexioned',\n",
       " 799: 'eats',\n",
       " 800: \"'em\",\n",
       " 801: 'afore',\n",
       " 802: 'resolved',\n",
       " 803: 'noise',\n",
       " 804: 'offing',\n",
       " 805: 'shaggy',\n",
       " 806: 'woollen',\n",
       " 807: 'stiff',\n",
       " 808: 'labrador',\n",
       " 809: 'wake',\n",
       " 810: 'bad',\n",
       " 811: 'mounted',\n",
       " 812: 'generally',\n",
       " 813: 'shipmates',\n",
       " 814: 'become',\n",
       " 815: 'height',\n",
       " 816: 'seem',\n",
       " 817: 'plan',\n",
       " 818: 'private',\n",
       " 819: 'unknown',\n",
       " 820: 'apartment',\n",
       " 821: 'hammock',\n",
       " 822: 'linen',\n",
       " 823: 'home',\n",
       " 824: 'hole',\n",
       " 825: 'mattress',\n",
       " 826: 'planing',\n",
       " 827: 'knot',\n",
       " 828: 'near',\n",
       " 829: 'sake',\n",
       " 830: 'chair',\n",
       " 831: 'narrow',\n",
       " 832: 'leaving',\n",
       " 833: 'interval',\n",
       " 834: 'met',\n",
       " 835: 'inside',\n",
       " 836: 'violent',\n",
       " 837: 'ones',\n",
       " 838: 'comprehension',\n",
       " 839: 'bird',\n",
       " 840: 'airley',\n",
       " 841: 'airth',\n",
       " 842: 'engaged',\n",
       " 843: 'whittling',\n",
       " 844: 'guess',\n",
       " 845: 'done',\n",
       " 846: 'break',\n",
       " 847: 'broke',\n",
       " 848: 'understand',\n",
       " 849: 'certain',\n",
       " 850: 'demand',\n",
       " 851: 'selling',\n",
       " 852: 'sir',\n",
       " 853: 'string',\n",
       " 854: 'mystery',\n",
       " 855: 'showed',\n",
       " 856: 'dangerous',\n",
       " 857: 'flukes',\n",
       " 858: 'slept',\n",
       " 859: 'big',\n",
       " 860: 'sam',\n",
       " 861: 'lighted',\n",
       " 862: 'wo',\n",
       " 863: 'stairs',\n",
       " 864: 'placing',\n",
       " 865: 'double',\n",
       " 866: 'eyeing',\n",
       " 867: 'belonging',\n",
       " 868: 'papered',\n",
       " 869: 'parcel',\n",
       " 870: 'tried',\n",
       " 871: 'satisfactory',\n",
       " 872: 'concerning',\n",
       " 873: 'edges',\n",
       " 874: 'stuck',\n",
       " 875: 'gave',\n",
       " 876: 'neck',\n",
       " 877: 'sleeves',\n",
       " 878: 'undressed',\n",
       " 879: 'remembering',\n",
       " 880: 'jumped',\n",
       " 881: 'pantaloons',\n",
       " 882: 'blowing',\n",
       " 883: 'doze',\n",
       " 884: 'pretty',\n",
       " 885: 'heavy',\n",
       " 886: 'save',\n",
       " 887: 'infernal',\n",
       " 888: 'peddler',\n",
       " 889: 'yellow',\n",
       " 890: 'colour',\n",
       " 891: 'dreadfully',\n",
       " 892: 'sticking',\n",
       " 893: 'cheeks',\n",
       " 894: 'truth',\n",
       " 895: 'remembered',\n",
       " 896: 'whaleman',\n",
       " 897: 'tattooed',\n",
       " 898: 'concluded',\n",
       " 899: 'lying',\n",
       " 900: 'tanning',\n",
       " 901: 'hot',\n",
       " 902: 'produced',\n",
       " 903: 'beaver',\n",
       " 904: 'singing',\n",
       " 905: 'bolted',\n",
       " 906: 'arms',\n",
       " 907: 'running',\n",
       " 908: 'curious',\n",
       " 909: 'hunch',\n",
       " 910: 'congo',\n",
       " 911: 'ill',\n",
       " 912: 'takes',\n",
       " 913: 'biscuit',\n",
       " 914: 'top',\n",
       " 915: 'lamp',\n",
       " 916: 'succeeded',\n",
       " 917: 'polite',\n",
       " 918: 'guttural',\n",
       " 919: 'pagan',\n",
       " 920: 'spell',\n",
       " 921: 'tobacco',\n",
       " 922: 'grunt',\n",
       " 923: 'whatever',\n",
       " 924: 'ee',\n",
       " 925: 'horrid',\n",
       " 926: 'pipe',\n",
       " 927: 'smoking',\n",
       " 928: 'complied',\n",
       " 929: 'patchwork',\n",
       " 930: 'figure',\n",
       " 931: 'shade',\n",
       " 932: 'quilt',\n",
       " 933: 'hugging',\n",
       " 934: 'sensations',\n",
       " 935: 'explain',\n",
       " 936: 'remember',\n",
       " 937: 'circumstance',\n",
       " 938: 'reality',\n",
       " 939: 'stepmother',\n",
       " 940: 'sixteen',\n",
       " 941: 'abed',\n",
       " 942: 'troubled',\n",
       " 943: 'supernatural',\n",
       " 944: 'ages',\n",
       " 945: 'awful',\n",
       " 946: 'consciousness',\n",
       " 947: 'observing',\n",
       " 948: 'creature',\n",
       " 949: 'dress',\n",
       " 950: 'watching',\n",
       " 951: 'probably',\n",
       " 952: 'toilet',\n",
       " 953: 'wrapped',\n",
       " 954: 'precisely',\n",
       " 955: 'jacket',\n",
       " 956: 'call',\n",
       " 957: 'shore',\n",
       " 958: 'watery',\n",
       " 959: 'driving',\n",
       " 960: 'spleen',\n",
       " 961: 'regulating',\n",
       " 962: 'circulation',\n",
       " 963: 'growing',\n",
       " 964: 'grim',\n",
       " 965: 'drizzly',\n",
       " 966: 'november',\n",
       " 967: 'bringing',\n",
       " 968: 'rear',\n",
       " 969: 'funeral',\n",
       " 970: 'meet',\n",
       " 971: 'hypos',\n",
       " 972: 'upper',\n",
       " 973: 'moral',\n",
       " 974: 'principle',\n",
       " 975: 'prevent',\n",
       " 976: 'deliberately',\n",
       " 977: 'stepping',\n",
       " 978: 'methodically',\n",
       " 979: 'knocking',\n",
       " 980: 'hats',\n",
       " 981: 'substitute',\n",
       " 982: 'pistol',\n",
       " 983: 'ball',\n",
       " 984: 'philosophical',\n",
       " 985: 'flourish',\n",
       " 986: 'cato',\n",
       " 987: 'throws',\n",
       " 988: 'surprising',\n",
       " 989: 'cherish',\n",
       " 990: 'feelings',\n",
       " 991: 'insular',\n",
       " 992: 'manhattoes',\n",
       " 993: 'belted',\n",
       " 994: 'wharves',\n",
       " 995: 'isles',\n",
       " 996: 'coral',\n",
       " 997: 'reefs',\n",
       " 998: 'commerce',\n",
       " 999: 'surrounds',\n",
       " 1000: 'surf',\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'and': 3,\n",
       " 'of': 4,\n",
       " 'i': 5,\n",
       " 'to': 6,\n",
       " 'in': 7,\n",
       " 'it': 8,\n",
       " 'that': 9,\n",
       " 'he': 10,\n",
       " 'his': 11,\n",
       " 'was': 12,\n",
       " 'but': 13,\n",
       " 'me': 14,\n",
       " 'with': 15,\n",
       " 'as': 16,\n",
       " 'at': 17,\n",
       " 'this': 18,\n",
       " 'you': 19,\n",
       " 'is': 20,\n",
       " 'all': 21,\n",
       " 'for': 22,\n",
       " 'my': 23,\n",
       " 'on': 24,\n",
       " 'be': 25,\n",
       " \"'s\": 26,\n",
       " 'not': 27,\n",
       " 'from': 28,\n",
       " 'there': 29,\n",
       " 'one': 30,\n",
       " 'up': 31,\n",
       " 'what': 32,\n",
       " 'him': 33,\n",
       " 'so': 34,\n",
       " 'bed': 35,\n",
       " 'now': 36,\n",
       " 'about': 37,\n",
       " 'no': 38,\n",
       " 'into': 39,\n",
       " 'by': 40,\n",
       " 'were': 41,\n",
       " 'out': 42,\n",
       " 'or': 43,\n",
       " 'harpooneer': 44,\n",
       " 'had': 45,\n",
       " 'then': 46,\n",
       " 'have': 47,\n",
       " 'an': 48,\n",
       " 'upon': 49,\n",
       " 'little': 50,\n",
       " 'some': 51,\n",
       " 'old': 52,\n",
       " 'like': 53,\n",
       " 'if': 54,\n",
       " 'they': 55,\n",
       " 'would': 56,\n",
       " 'do': 57,\n",
       " 'over': 58,\n",
       " 'landlord': 59,\n",
       " 'thought': 60,\n",
       " 'room': 61,\n",
       " 'when': 62,\n",
       " 'could': 63,\n",
       " \"n't\": 64,\n",
       " 'night': 65,\n",
       " 'here': 66,\n",
       " 'head': 67,\n",
       " 'such': 68,\n",
       " 'which': 69,\n",
       " 'man': 70,\n",
       " 'did': 71,\n",
       " 'sea': 72,\n",
       " 'time': 73,\n",
       " 'other': 74,\n",
       " 'very': 75,\n",
       " 'go': 76,\n",
       " 'these': 77,\n",
       " 'more': 78,\n",
       " 'though': 79,\n",
       " 'first': 80,\n",
       " 'sort': 81,\n",
       " 'said': 82,\n",
       " 'last': 83,\n",
       " 'down': 84,\n",
       " 'most': 85,\n",
       " 'been': 86,\n",
       " 'never': 87,\n",
       " 'your': 88,\n",
       " 'them': 89,\n",
       " 'must': 90,\n",
       " 'tell': 91,\n",
       " 'much': 92,\n",
       " 'good': 93,\n",
       " 'see': 94,\n",
       " 'off': 95,\n",
       " 'myself': 96,\n",
       " 'are': 97,\n",
       " 'yet': 98,\n",
       " 'sleep': 99,\n",
       " 'who': 100,\n",
       " 'seemed': 101,\n",
       " 'light': 102,\n",
       " 'way': 103,\n",
       " 'their': 104,\n",
       " 'just': 105,\n",
       " 'being': 106,\n",
       " 'than': 107,\n",
       " 'place': 108,\n",
       " 'queequeg': 109,\n",
       " 'great': 110,\n",
       " 'long': 111,\n",
       " 'before': 112,\n",
       " 'get': 113,\n",
       " 'round': 114,\n",
       " 'where': 115,\n",
       " 'still': 116,\n",
       " 'any': 117,\n",
       " 'too': 118,\n",
       " 'only': 119,\n",
       " 'door': 120,\n",
       " 'can': 121,\n",
       " 'himself': 122,\n",
       " 'heads': 123,\n",
       " 'come': 124,\n",
       " 'ever': 125,\n",
       " 'two': 126,\n",
       " 'enough': 127,\n",
       " 'made': 128,\n",
       " 'how': 129,\n",
       " 'hand': 130,\n",
       " 'same': 131,\n",
       " 'looking': 132,\n",
       " 'something': 133,\n",
       " 'may': 134,\n",
       " \"'\": 135,\n",
       " 'almost': 136,\n",
       " 'say': 137,\n",
       " 'should': 138,\n",
       " 'side': 139,\n",
       " 'why': 140,\n",
       " 'own': 141,\n",
       " 'we': 142,\n",
       " 'new': 143,\n",
       " 'again': 144,\n",
       " 'came': 145,\n",
       " 'arm': 146,\n",
       " 'house': 147,\n",
       " 'away': 148,\n",
       " 'might': 149,\n",
       " 'nothing': 150,\n",
       " 'take': 151,\n",
       " 'towards': 152,\n",
       " 'will': 153,\n",
       " 'under': 154,\n",
       " 'going': 155,\n",
       " 'make': 156,\n",
       " 'whale': 157,\n",
       " 'stood': 158,\n",
       " 'boots': 159,\n",
       " 'ye': 160,\n",
       " 'back': 161,\n",
       " \"'ll\": 162,\n",
       " 'tomahawk': 163,\n",
       " 'part': 164,\n",
       " 'world': 165,\n",
       " 'soon': 166,\n",
       " 'water': 167,\n",
       " 'against': 168,\n",
       " 'those': 169,\n",
       " 'between': 170,\n",
       " 'after': 171,\n",
       " 'whaling': 172,\n",
       " 'lay': 173,\n",
       " 'took': 174,\n",
       " 'half': 175,\n",
       " 'began': 176,\n",
       " 'face': 177,\n",
       " 'streets': 178,\n",
       " 'land': 179,\n",
       " 'better': 180,\n",
       " 'once': 181,\n",
       " 'voyage': 182,\n",
       " 'give': 183,\n",
       " 'rather': 184,\n",
       " 'well': 185,\n",
       " 'however': 186,\n",
       " 'else': 187,\n",
       " 'heard': 188,\n",
       " 'put': 189,\n",
       " 'stop': 190,\n",
       " 'dark': 191,\n",
       " 'went': 192,\n",
       " 'black': 193,\n",
       " 'window': 194,\n",
       " 'cannibal': 195,\n",
       " 'fire': 196,\n",
       " 'every': 197,\n",
       " 'ship': 198,\n",
       " 'stand': 199,\n",
       " 'strange': 200,\n",
       " 'without': 201,\n",
       " 'feet': 202,\n",
       " 'whether': 203,\n",
       " 'because': 204,\n",
       " 'eyes': 205,\n",
       " 'think': 206,\n",
       " 'thinks': 207,\n",
       " 'idea': 208,\n",
       " 'bag': 209,\n",
       " 'nantucket': 210,\n",
       " 'late': 211,\n",
       " 'cold': 212,\n",
       " 'our': 213,\n",
       " 'found': 214,\n",
       " 'full': 215,\n",
       " 'morning': 216,\n",
       " 'sleeping': 217,\n",
       " 'got': 218,\n",
       " 'mind': 219,\n",
       " 'her': 220,\n",
       " 'right': 221,\n",
       " 'its': 222,\n",
       " 'look': 223,\n",
       " 'town': 224,\n",
       " 'south': 225,\n",
       " 'does': 226,\n",
       " 'let': 227,\n",
       " 'set': 228,\n",
       " 'yourself': 229,\n",
       " 'image': 230,\n",
       " 'saw': 231,\n",
       " 'am': 232,\n",
       " 'besides': 233,\n",
       " 'sailor': 234,\n",
       " 'seas': 235,\n",
       " 'rolled': 236,\n",
       " 'till': 237,\n",
       " 'day': 238,\n",
       " 'sign': 239,\n",
       " 'looked': 240,\n",
       " 'hard': 241,\n",
       " 'moment': 242,\n",
       " 'corner': 243,\n",
       " 'entry': 244,\n",
       " 'four': 245,\n",
       " 'wall': 246,\n",
       " 'savage': 247,\n",
       " 'table': 248,\n",
       " 'indeed': 249,\n",
       " 'bench': 250,\n",
       " 'chest': 251,\n",
       " 'while': 252,\n",
       " 'stranger': 253,\n",
       " 'possible': 254,\n",
       " 'feeling': 255,\n",
       " 'floor': 256,\n",
       " 'squares': 257,\n",
       " 'hat': 258,\n",
       " 'particular': 259,\n",
       " 'having': 260,\n",
       " 'years': 261,\n",
       " 'harpoon': 262,\n",
       " 'ishmael': 263,\n",
       " 'whenever': 264,\n",
       " 'mouth': 265,\n",
       " 'high': 266,\n",
       " 'knew': 267,\n",
       " 'men': 268,\n",
       " 'hours': 269,\n",
       " 'green': 270,\n",
       " 'bit': 271,\n",
       " 'within': 272,\n",
       " 'picture': 273,\n",
       " 'told': 274,\n",
       " 'story': 275,\n",
       " 'mean': 276,\n",
       " 'speak': 277,\n",
       " 'order': 278,\n",
       " 'making': 279,\n",
       " 'even': 280,\n",
       " 'perhaps': 281,\n",
       " 'things': 282,\n",
       " 'answer': 283,\n",
       " 'parts': 284,\n",
       " 'wild': 285,\n",
       " 'reason': 286,\n",
       " 'young': 287,\n",
       " 'craft': 288,\n",
       " 'business': 289,\n",
       " 'dead': 290,\n",
       " 'another': 291,\n",
       " 'middle': 292,\n",
       " 'sure': 293,\n",
       " 'candle': 294,\n",
       " 'presently': 295,\n",
       " 'low': 296,\n",
       " 'turned': 297,\n",
       " 'teeth': 298,\n",
       " 'dim': 299,\n",
       " 'euroclydon': 300,\n",
       " 'kept': 301,\n",
       " 'glass': 302,\n",
       " 'afterwards': 303,\n",
       " 'large': 304,\n",
       " 'three': 305,\n",
       " 'telling': 306,\n",
       " 'getting': 307,\n",
       " 'small': 308,\n",
       " 'next': 309,\n",
       " 'seeing': 310,\n",
       " 'sell': 311,\n",
       " 'felt': 312,\n",
       " 'sun': 313,\n",
       " 'money': 314,\n",
       " 'sail': 315,\n",
       " 'coffin': 316,\n",
       " 'especially': 317,\n",
       " 'street': 318,\n",
       " 'city': 319,\n",
       " 'few': 320,\n",
       " 'previous': 321,\n",
       " 'sight': 322,\n",
       " 'days': 323,\n",
       " 'straight': 324,\n",
       " 'nigh': 325,\n",
       " 'legs': 326,\n",
       " 'try': 327,\n",
       " 'yes': 328,\n",
       " 'unless': 329,\n",
       " 'poor': 330,\n",
       " 'coat': 331,\n",
       " 'passenger': 332,\n",
       " 'taking': 333,\n",
       " 'true': 334,\n",
       " 'thing': 335,\n",
       " 'ai': 336,\n",
       " 'always': 337,\n",
       " 'us': 338,\n",
       " 'really': 339,\n",
       " 'marvellous': 340,\n",
       " 'heaven': 341,\n",
       " 'air': 342,\n",
       " 'far': 343,\n",
       " 'second': 344,\n",
       " 'many': 345,\n",
       " 'has': 346,\n",
       " 'unaccountable': 347,\n",
       " 'grand': 348,\n",
       " 'jolly': 349,\n",
       " 'open': 350,\n",
       " 'shirt': 351,\n",
       " 'cape': 352,\n",
       " 'bedford': 353,\n",
       " 'fine': 354,\n",
       " 'further': 355,\n",
       " 'ice': 356,\n",
       " 'frost': 357,\n",
       " 'foot': 358,\n",
       " 'wide': 359,\n",
       " 'white': 360,\n",
       " 'tall': 361,\n",
       " 'i.': 362,\n",
       " 'wooden': 363,\n",
       " 'worse': 364,\n",
       " 'death': 365,\n",
       " 'mine': 366,\n",
       " 'lazarus': 367,\n",
       " 'keep': 368,\n",
       " 'along': 369,\n",
       " 'hung': 370,\n",
       " 'throwing': 371,\n",
       " 'centre': 372,\n",
       " 'rest': 373,\n",
       " 'fact': 374,\n",
       " 'hair': 375,\n",
       " 'broken': 376,\n",
       " 'kill': 377,\n",
       " 'through': 378,\n",
       " 'chimney': 379,\n",
       " 'fancy': 380,\n",
       " 'bar': 381,\n",
       " 'trying': 382,\n",
       " 'dumplings': 383,\n",
       " 'heavens': 384,\n",
       " 'manner': 385,\n",
       " 'devil': 386,\n",
       " 'together': 387,\n",
       " 'seen': 388,\n",
       " 'deal': 389,\n",
       " 'know': 390,\n",
       " 'skin': 391,\n",
       " 'ca': 392,\n",
       " 'shavings': 393,\n",
       " 'peddling': 394,\n",
       " 'sunday': 395,\n",
       " 'counterpane': 396,\n",
       " 'mat': 397,\n",
       " 'christian': 398,\n",
       " 'commenced': 399,\n",
       " 'thinking': 400,\n",
       " 'similar': 401,\n",
       " 'afraid': 402,\n",
       " 'length': 403,\n",
       " 'idol': 404,\n",
       " 'e': 405,\n",
       " 'sabbee': 406,\n",
       " 'waking': 407,\n",
       " 'ago': 408,\n",
       " 'find': 409,\n",
       " 'damp': 410,\n",
       " 'soul': 411,\n",
       " 'strong': 412,\n",
       " 'account': 413,\n",
       " 'sword': 414,\n",
       " 'quietly': 415,\n",
       " 'degree': 416,\n",
       " 'left': 417,\n",
       " 'around': 418,\n",
       " 'fixed': 419,\n",
       " 'ships': 420,\n",
       " 'miles': 421,\n",
       " 'country': 422,\n",
       " 'stream': 423,\n",
       " 'lead': 424,\n",
       " 'american': 425,\n",
       " 'artist': 426,\n",
       " 'each': 427,\n",
       " 'goes': 428,\n",
       " 'deep': 429,\n",
       " 'distant': 430,\n",
       " 'winds': 431,\n",
       " 'blue': 432,\n",
       " 'among': 433,\n",
       " 'suddenly': 434,\n",
       " 'feel': 435,\n",
       " 'meaning': 436,\n",
       " 'phantom': 437,\n",
       " 'life': 438,\n",
       " 'passengers': 439,\n",
       " 'nor': 440,\n",
       " 'kind': 441,\n",
       " 'quite': 442,\n",
       " 'care': 443,\n",
       " 'board': 444,\n",
       " 'somehow': 445,\n",
       " 'broiled': 446,\n",
       " 'mast': 447,\n",
       " 'sense': 448,\n",
       " 'knowing': 449,\n",
       " 'either': 450,\n",
       " 'passed': 451,\n",
       " 'hands': 452,\n",
       " 'paying': 453,\n",
       " 'pay': 454,\n",
       " 'penny': 455,\n",
       " 'sailors': 456,\n",
       " 'exactly': 457,\n",
       " 'short': 458,\n",
       " 'easy': 459,\n",
       " 'portentous': 460,\n",
       " 'island': 461,\n",
       " 'nameless': 462,\n",
       " 'sounds': 463,\n",
       " 'since': 464,\n",
       " 'snow': 465,\n",
       " 'saturday': 466,\n",
       " 'matter': 467,\n",
       " 'red': 468,\n",
       " 'partly': 469,\n",
       " 'ere': 470,\n",
       " 'became': 471,\n",
       " 'meanwhile': 472,\n",
       " 'pocket': 473,\n",
       " 'darkness': 474,\n",
       " 'fish': 475,\n",
       " 'inn': 476,\n",
       " 'watch': 477,\n",
       " 'broad': 478,\n",
       " 'entering': 479,\n",
       " 'ha': 480,\n",
       " 'ashes': 481,\n",
       " 'opened': 482,\n",
       " 'spouter': 483,\n",
       " 'name': 484,\n",
       " 'suppose': 485,\n",
       " 'quiet': 486,\n",
       " 'best': 487,\n",
       " 'tempestuous': 488,\n",
       " 'says': 489,\n",
       " 'thou': 490,\n",
       " 'both': 491,\n",
       " 'occurred': 492,\n",
       " 'dives': 493,\n",
       " 'holding': 494,\n",
       " 'frozen': 495,\n",
       " 'altogether': 496,\n",
       " 'plain': 497,\n",
       " 'whom': 498,\n",
       " 'clean': 499,\n",
       " 'human': 500,\n",
       " 'entered': 501,\n",
       " 'wrinkled': 502,\n",
       " 'shelf': 503,\n",
       " 'jonah': 504,\n",
       " 'blanket': 505,\n",
       " \"goin'\": 506,\n",
       " 'bitter': 507,\n",
       " 'supper': 508,\n",
       " 'sat': 509,\n",
       " 'settle': 510,\n",
       " 'chap': 511,\n",
       " 'help': 512,\n",
       " 'spend': 513,\n",
       " 'landed': 514,\n",
       " 'standing': 515,\n",
       " 'held': 516,\n",
       " 'somewhat': 517,\n",
       " 'sober': 518,\n",
       " 'whole': 519,\n",
       " 'dam': 520,\n",
       " 'brown': 521,\n",
       " 'bulkington': 522,\n",
       " \"o'clock\": 523,\n",
       " 'none': 524,\n",
       " 'coming': 525,\n",
       " \"'ve\": 526,\n",
       " 'wait': 527,\n",
       " 'plane': 528,\n",
       " 'saying': 529,\n",
       " 'grinning': 530,\n",
       " 'placed': 531,\n",
       " 'shouted': 532,\n",
       " 'bedfellow': 533,\n",
       " 'zealand': 534,\n",
       " 'sal': 535,\n",
       " 'wash': 536,\n",
       " 'thrown': 537,\n",
       " 'purplish': 538,\n",
       " 'turn': 539,\n",
       " 'completely': 540,\n",
       " 'fear': 541,\n",
       " 'grego': 542,\n",
       " 'baby': 543,\n",
       " 'slowly': 544,\n",
       " 'civilized': 545,\n",
       " 'purse': 546,\n",
       " 'monkey': 547,\n",
       " 'involuntarily': 548,\n",
       " 'pausing': 549,\n",
       " 'warehouses': 550,\n",
       " 'requires': 551,\n",
       " 'people': 552,\n",
       " 'nearly': 553,\n",
       " 'ocean': 554,\n",
       " 'indian': 555,\n",
       " 'waterward': 556,\n",
       " 'battery': 557,\n",
       " 'noble': 558,\n",
       " 'washed': 559,\n",
       " 'crowds': 560,\n",
       " 'sabbath': 561,\n",
       " 'afternoon': 562,\n",
       " 'thence': 563,\n",
       " 'silent': 564,\n",
       " 'thousands': 565,\n",
       " 'reveries': 566,\n",
       " 'leaning': 567,\n",
       " 'seated': 568,\n",
       " 'bulwarks': 569,\n",
       " 'aloft': 570,\n",
       " 'week': 571,\n",
       " 'plaster': 572,\n",
       " 'gone': 573,\n",
       " 'bound': 574,\n",
       " 'content': 575,\n",
       " 'yonder': 576,\n",
       " 'falling': 577,\n",
       " 'north': 578,\n",
       " 'please': 579,\n",
       " 'ten': 580,\n",
       " 'leaves': 581,\n",
       " 'magic': 582,\n",
       " 'plunged': 583,\n",
       " 'metaphysical': 584,\n",
       " 'chief': 585,\n",
       " 'trunk': 586,\n",
       " 'meadow': 587,\n",
       " 'smoke': 588,\n",
       " 'reaching': 589,\n",
       " 'hill': 590,\n",
       " 'thus': 591,\n",
       " 'pine': 592,\n",
       " 'sighs': 593,\n",
       " 'shepherd': 594,\n",
       " 'june': 595,\n",
       " 'scores': 596,\n",
       " 'thousand': 597,\n",
       " 'sadly': 598,\n",
       " 'robust': 599,\n",
       " 'healthy': 600,\n",
       " 'boy': 601,\n",
       " 'crazy': 602,\n",
       " 'hold': 603,\n",
       " 'holy': 604,\n",
       " 'brother': 605,\n",
       " 'ourselves': 606,\n",
       " 'begin': 607,\n",
       " 'grow': 608,\n",
       " 'inferred': 609,\n",
       " 'needs': 610,\n",
       " \"don't\": 611,\n",
       " 'themselves': 612,\n",
       " 'commodore': 613,\n",
       " 'captain': 614,\n",
       " 'cook': 615,\n",
       " 'glory': 616,\n",
       " 'whatsoever': 617,\n",
       " 'confess': 618,\n",
       " 'officer': 619,\n",
       " 'respectfully': 620,\n",
       " 'horse': 621,\n",
       " 'huge': 622,\n",
       " 'houses': 623,\n",
       " 'forecastle': 624,\n",
       " 'jump': 625,\n",
       " 'spar': 626,\n",
       " 'particularly': 627,\n",
       " 'putting': 628,\n",
       " 'tar': 629,\n",
       " 'schoolmaster': 630,\n",
       " 'boys': 631,\n",
       " 'transition': 632,\n",
       " 'grin': 633,\n",
       " 'bear': 634,\n",
       " 'hunks': 635,\n",
       " 'sweep': 636,\n",
       " 'weighed': 637,\n",
       " 'anything': 638,\n",
       " 'less': 639,\n",
       " 'thump': 640,\n",
       " 'point': 641,\n",
       " 'view': 642,\n",
       " 'single': 643,\n",
       " 'difference': 644,\n",
       " 'act': 645,\n",
       " 'uncomfortable': 646,\n",
       " 'compare': 647,\n",
       " 'earthly': 648,\n",
       " 'enter': 649,\n",
       " 'deck': 650,\n",
       " 'quarter': 651,\n",
       " 'leaders': 652,\n",
       " 'smelt': 653,\n",
       " 'fates': 654,\n",
       " 'doubtless': 655,\n",
       " 'formed': 656,\n",
       " 'run': 657,\n",
       " 'stage': 658,\n",
       " 'shabby': 659,\n",
       " 'others': 660,\n",
       " 'circumstances': 661,\n",
       " 'motives': 662,\n",
       " 'various': 663,\n",
       " 'mysterious': 664,\n",
       " 'curiosity': 665,\n",
       " 'tormented': 666,\n",
       " 'everlasting': 667,\n",
       " 'wonder': 668,\n",
       " 'purpose': 669,\n",
       " 'floated': 670,\n",
       " 'stuffed': 671,\n",
       " 'horn': 672,\n",
       " 'arrived': 673,\n",
       " 'offer': 674,\n",
       " 'following': 675,\n",
       " 'embark': 676,\n",
       " 'pleased': 677,\n",
       " 'original': 678,\n",
       " 'stranded': 679,\n",
       " 'leviathan': 680,\n",
       " 'whales': 681,\n",
       " 'nay': 682,\n",
       " 'dismal': 683,\n",
       " 'wherever': 684,\n",
       " 'dreary': 685,\n",
       " 'crossed': 686,\n",
       " 'expensive': 687,\n",
       " 'bright': 688,\n",
       " 'windows': 689,\n",
       " 'packed': 690,\n",
       " 'inches': 691,\n",
       " 'thick': 692,\n",
       " 'hear': 693,\n",
       " 'tinkling': 694,\n",
       " 'glasses': 695,\n",
       " 'blackness': 696,\n",
       " 'moving': 697,\n",
       " 'hour': 698,\n",
       " 'proved': 699,\n",
       " 'meant': 700,\n",
       " 'public': 701,\n",
       " 'box': 702,\n",
       " 'flying': 703,\n",
       " 'harpoons': 704,\n",
       " 'trap': 705,\n",
       " 'loud': 706,\n",
       " 'voice': 707,\n",
       " 'sitting': 708,\n",
       " 'beyond': 709,\n",
       " 'negro': 710,\n",
       " 'creaking': 711,\n",
       " 'swinging': 712,\n",
       " 'painting': 713,\n",
       " 'representing': 714,\n",
       " 'connexion': 715,\n",
       " 'peter': 716,\n",
       " 'itself': 717,\n",
       " 'carted': 718,\n",
       " 'burnt': 719,\n",
       " 'spot': 720,\n",
       " 'queer': 721,\n",
       " 'gable': 722,\n",
       " 'ended': 723,\n",
       " 'sharp': 724,\n",
       " 'wind': 725,\n",
       " 'howling': 726,\n",
       " 'nevertheless': 727,\n",
       " 'called': 728,\n",
       " 'outside': 729,\n",
       " 'passage': 730,\n",
       " 'body': 731,\n",
       " 'curbstone': 732,\n",
       " 'corn': 733,\n",
       " 'pooh': 734,\n",
       " 'northern': 735,\n",
       " 'lights': 736,\n",
       " 'summer': 737,\n",
       " 'lengthwise': 738,\n",
       " 'gods': 739,\n",
       " 'lie': 740,\n",
       " 'plenty': 741,\n",
       " 'study': 742,\n",
       " 'series': 743,\n",
       " 'arrive': 744,\n",
       " 'shadows': 745,\n",
       " 'dint': 746,\n",
       " 'conclusion': 747,\n",
       " 'confounded': 748,\n",
       " 'limber': 749,\n",
       " 'truly': 750,\n",
       " 'drive': 751,\n",
       " 'unimaginable': 752,\n",
       " 'midnight': 753,\n",
       " 'unnatural': 754,\n",
       " 'breaking': 755,\n",
       " 'design': 756,\n",
       " 'opposite': 757,\n",
       " 'monstrous': 758,\n",
       " 'knots': 759,\n",
       " 'vast': 760,\n",
       " 'handle': 761,\n",
       " 'wondered': 762,\n",
       " 'mixed': 763,\n",
       " 'deformed': 764,\n",
       " 'flung': 765,\n",
       " 'iron': 766,\n",
       " 'arched': 767,\n",
       " 'cut': 768,\n",
       " 'times': 769,\n",
       " 'beneath': 770,\n",
       " 'covered': 771,\n",
       " 'gathered': 772,\n",
       " 'stands': 773,\n",
       " 'rude': 774,\n",
       " 'bone': 775,\n",
       " 'abominable': 776,\n",
       " 'measure': 777,\n",
       " 'seamen': 778,\n",
       " 'skrimshander': 779,\n",
       " 'sought': 780,\n",
       " 'added': 781,\n",
       " 'forehead': 782,\n",
       " 'objections': 783,\n",
       " \"'d\": 784,\n",
       " 'used': 785,\n",
       " 'depend': 786,\n",
       " 'decent': 787,\n",
       " 'want': 788,\n",
       " 'ready': 789,\n",
       " 'working': 790,\n",
       " 'space': 791,\n",
       " 'lips': 792,\n",
       " 'fingers': 793,\n",
       " 'fare': 794,\n",
       " 'fellow': 795,\n",
       " 'nightmare': 796,\n",
       " 'nt': 797,\n",
       " 'complexioned': 798,\n",
       " 'eats': 799,\n",
       " \"'em\": 800,\n",
       " 'afore': 801,\n",
       " 'resolved': 802,\n",
       " 'noise': 803,\n",
       " 'offing': 804,\n",
       " 'shaggy': 805,\n",
       " 'woollen': 806,\n",
       " 'stiff': 807,\n",
       " 'labrador': 808,\n",
       " 'wake': 809,\n",
       " 'bad': 810,\n",
       " 'mounted': 811,\n",
       " 'generally': 812,\n",
       " 'shipmates': 813,\n",
       " 'become': 814,\n",
       " 'height': 815,\n",
       " 'seem': 816,\n",
       " 'plan': 817,\n",
       " 'private': 818,\n",
       " 'unknown': 819,\n",
       " 'apartment': 820,\n",
       " 'hammock': 821,\n",
       " 'linen': 822,\n",
       " 'home': 823,\n",
       " 'hole': 824,\n",
       " 'mattress': 825,\n",
       " 'planing': 826,\n",
       " 'knot': 827,\n",
       " 'near': 828,\n",
       " 'sake': 829,\n",
       " 'chair': 830,\n",
       " 'narrow': 831,\n",
       " 'leaving': 832,\n",
       " 'interval': 833,\n",
       " 'met': 834,\n",
       " 'inside': 835,\n",
       " 'violent': 836,\n",
       " 'ones': 837,\n",
       " 'comprehension': 838,\n",
       " 'bird': 839,\n",
       " 'airley': 840,\n",
       " 'airth': 841,\n",
       " 'engaged': 842,\n",
       " 'whittling': 843,\n",
       " 'guess': 844,\n",
       " 'done': 845,\n",
       " 'break': 846,\n",
       " 'broke': 847,\n",
       " 'understand': 848,\n",
       " 'certain': 849,\n",
       " 'demand': 850,\n",
       " 'selling': 851,\n",
       " 'sir': 852,\n",
       " 'string': 853,\n",
       " 'mystery': 854,\n",
       " 'showed': 855,\n",
       " 'dangerous': 856,\n",
       " 'flukes': 857,\n",
       " 'slept': 858,\n",
       " 'big': 859,\n",
       " 'sam': 860,\n",
       " 'lighted': 861,\n",
       " 'wo': 862,\n",
       " 'stairs': 863,\n",
       " 'placing': 864,\n",
       " 'double': 865,\n",
       " 'eyeing': 866,\n",
       " 'belonging': 867,\n",
       " 'papered': 868,\n",
       " 'parcel': 869,\n",
       " 'tried': 870,\n",
       " 'satisfactory': 871,\n",
       " 'concerning': 872,\n",
       " 'edges': 873,\n",
       " 'stuck': 874,\n",
       " 'gave': 875,\n",
       " 'neck': 876,\n",
       " 'sleeves': 877,\n",
       " 'undressed': 878,\n",
       " 'remembering': 879,\n",
       " 'jumped': 880,\n",
       " 'pantaloons': 881,\n",
       " 'blowing': 882,\n",
       " 'doze': 883,\n",
       " 'pretty': 884,\n",
       " 'heavy': 885,\n",
       " 'save': 886,\n",
       " 'infernal': 887,\n",
       " 'peddler': 888,\n",
       " 'yellow': 889,\n",
       " 'colour': 890,\n",
       " 'dreadfully': 891,\n",
       " 'sticking': 892,\n",
       " 'cheeks': 893,\n",
       " 'truth': 894,\n",
       " 'remembered': 895,\n",
       " 'whaleman': 896,\n",
       " 'tattooed': 897,\n",
       " 'concluded': 898,\n",
       " 'lying': 899,\n",
       " 'tanning': 900,\n",
       " 'hot': 901,\n",
       " 'produced': 902,\n",
       " 'beaver': 903,\n",
       " 'singing': 904,\n",
       " 'bolted': 905,\n",
       " 'arms': 906,\n",
       " 'running': 907,\n",
       " 'curious': 908,\n",
       " 'hunch': 909,\n",
       " 'congo': 910,\n",
       " 'ill': 911,\n",
       " 'takes': 912,\n",
       " 'biscuit': 913,\n",
       " 'top': 914,\n",
       " 'lamp': 915,\n",
       " 'succeeded': 916,\n",
       " 'polite': 917,\n",
       " 'guttural': 918,\n",
       " 'pagan': 919,\n",
       " 'spell': 920,\n",
       " 'tobacco': 921,\n",
       " 'grunt': 922,\n",
       " 'whatever': 923,\n",
       " 'ee': 924,\n",
       " 'horrid': 925,\n",
       " 'pipe': 926,\n",
       " 'smoking': 927,\n",
       " 'complied': 928,\n",
       " 'patchwork': 929,\n",
       " 'figure': 930,\n",
       " 'shade': 931,\n",
       " 'quilt': 932,\n",
       " 'hugging': 933,\n",
       " 'sensations': 934,\n",
       " 'explain': 935,\n",
       " 'remember': 936,\n",
       " 'circumstance': 937,\n",
       " 'reality': 938,\n",
       " 'stepmother': 939,\n",
       " 'sixteen': 940,\n",
       " 'abed': 941,\n",
       " 'troubled': 942,\n",
       " 'supernatural': 943,\n",
       " 'ages': 944,\n",
       " 'awful': 945,\n",
       " 'consciousness': 946,\n",
       " 'observing': 947,\n",
       " 'creature': 948,\n",
       " 'dress': 949,\n",
       " 'watching': 950,\n",
       " 'probably': 951,\n",
       " 'toilet': 952,\n",
       " 'wrapped': 953,\n",
       " 'precisely': 954,\n",
       " 'jacket': 955,\n",
       " 'call': 956,\n",
       " 'shore': 957,\n",
       " 'watery': 958,\n",
       " 'driving': 959,\n",
       " 'spleen': 960,\n",
       " 'regulating': 961,\n",
       " 'circulation': 962,\n",
       " 'growing': 963,\n",
       " 'grim': 964,\n",
       " 'drizzly': 965,\n",
       " 'november': 966,\n",
       " 'bringing': 967,\n",
       " 'rear': 968,\n",
       " 'funeral': 969,\n",
       " 'meet': 970,\n",
       " 'hypos': 971,\n",
       " 'upper': 972,\n",
       " 'moral': 973,\n",
       " 'principle': 974,\n",
       " 'prevent': 975,\n",
       " 'deliberately': 976,\n",
       " 'stepping': 977,\n",
       " 'methodically': 978,\n",
       " 'knocking': 979,\n",
       " 'hats': 980,\n",
       " 'substitute': 981,\n",
       " 'pistol': 982,\n",
       " 'ball': 983,\n",
       " 'philosophical': 984,\n",
       " 'flourish': 985,\n",
       " 'cato': 986,\n",
       " 'throws': 987,\n",
       " 'surprising': 988,\n",
       " 'cherish': 989,\n",
       " 'feelings': 990,\n",
       " 'insular': 991,\n",
       " 'manhattoes': 992,\n",
       " 'belted': 993,\n",
       " 'wharves': 994,\n",
       " 'isles': 995,\n",
       " 'coral': 996,\n",
       " 'reefs': 997,\n",
       " 'commerce': 998,\n",
       " 'surrounds': 999,\n",
       " 'surf': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('call', 27),\n",
       "             ('me', 2471),\n",
       "             ('ishmael', 133),\n",
       "             ('some', 758),\n",
       "             ('years', 135),\n",
       "             ('ago', 84),\n",
       "             ('never', 449),\n",
       "             ('mind', 164),\n",
       "             ('how', 321),\n",
       "             ('long', 374),\n",
       "             ('precisely', 37),\n",
       "             ('having', 142),\n",
       "             ('little', 767),\n",
       "             ('or', 950),\n",
       "             ('no', 1003),\n",
       "             ('money', 120),\n",
       "             ('in', 5647),\n",
       "             ('my', 1786),\n",
       "             ('purse', 71),\n",
       "             ('and', 9646),\n",
       "             ('nothing', 281),\n",
       "             ('particular', 152),\n",
       "             ('to', 6497),\n",
       "             ('interest', 24),\n",
       "             ('on', 1716),\n",
       "             ('shore', 26),\n",
       "             ('i', 7150),\n",
       "             ('thought', 676),\n",
       "             ('would', 702),\n",
       "             ('sail', 104),\n",
       "             ('about', 1014),\n",
       "             ('a', 10377),\n",
       "             ('see', 416),\n",
       "             ('the', 15540),\n",
       "             ('watery', 26),\n",
       "             ('part', 234),\n",
       "             ('of', 8287),\n",
       "             ('world', 234),\n",
       "             ('it', 4238),\n",
       "             ('is', 1950),\n",
       "             ('way', 390),\n",
       "             ('have', 806),\n",
       "             ('driving', 26),\n",
       "             ('off', 416),\n",
       "             ('spleen', 26),\n",
       "             ('regulating', 26),\n",
       "             ('circulation', 26),\n",
       "             ('whenever', 130),\n",
       "             ('find', 78),\n",
       "             ('myself', 416),\n",
       "             ('growing', 26),\n",
       "             ('grim', 26),\n",
       "             ('mouth', 130),\n",
       "             ('damp', 78),\n",
       "             ('drizzly', 26),\n",
       "             ('november', 26),\n",
       "             ('soul', 78),\n",
       "             ('involuntarily', 52),\n",
       "             ('pausing', 52),\n",
       "             ('before', 364),\n",
       "             ('coffin', 104),\n",
       "             ('warehouses', 52),\n",
       "             ('bringing', 26),\n",
       "             ('up', 1237),\n",
       "             ('rear', 26),\n",
       "             ('every', 182),\n",
       "             ('funeral', 26),\n",
       "             ('meet', 26),\n",
       "             ('especially', 104),\n",
       "             ('hypos', 26),\n",
       "             ('get', 364),\n",
       "             ('such', 572),\n",
       "             ('an', 806),\n",
       "             ('upper', 26),\n",
       "             ('hand', 312),\n",
       "             ('that', 3770),\n",
       "             ('requires', 52),\n",
       "             ('strong', 78),\n",
       "             ('moral', 26),\n",
       "             ('principle', 26),\n",
       "             ('prevent', 26),\n",
       "             ('from', 1508),\n",
       "             ('deliberately', 26),\n",
       "             ('stepping', 26),\n",
       "             ('into', 988),\n",
       "             ('street', 104),\n",
       "             ('methodically', 26),\n",
       "             ('knocking', 26),\n",
       "             ('people', 52),\n",
       "             (\"'s\", 1691),\n",
       "             ('hats', 26),\n",
       "             ('then', 832),\n",
       "             ('account', 78),\n",
       "             ('high', 130),\n",
       "             ('time', 520),\n",
       "             ('sea', 546),\n",
       "             ('as', 2366),\n",
       "             ('soon', 234),\n",
       "             ('can', 338),\n",
       "             ('this', 2158),\n",
       "             ('substitute', 26),\n",
       "             ('for', 1820),\n",
       "             ('pistol', 26),\n",
       "             ('ball', 26),\n",
       "             ('with', 2392),\n",
       "             ('philosophical', 26),\n",
       "             ('flourish', 26),\n",
       "             ('cato', 26),\n",
       "             ('throws', 26),\n",
       "             ('himself', 338),\n",
       "             ('upon', 780),\n",
       "             ('his', 3139),\n",
       "             ('sword', 78),\n",
       "             ('quietly', 78),\n",
       "             ('take', 260),\n",
       "             ('ship', 182),\n",
       "             ('there', 1456),\n",
       "             ('surprising', 26),\n",
       "             ('if', 728),\n",
       "             ('they', 728),\n",
       "             ('but', 2652),\n",
       "             ('knew', 130),\n",
       "             ('almost', 286),\n",
       "             ('all', 1872),\n",
       "             ('men', 130),\n",
       "             ('their', 390),\n",
       "             ('degree', 78),\n",
       "             ('other', 494),\n",
       "             ('cherish', 26),\n",
       "             ('very', 494),\n",
       "             ('nearly', 52),\n",
       "             ('same', 312),\n",
       "             ('feelings', 26),\n",
       "             ('towards', 260),\n",
       "             ('ocean', 52),\n",
       "             ('now', 1040),\n",
       "             ('your', 442),\n",
       "             ('insular', 26),\n",
       "             ('city', 104),\n",
       "             ('manhattoes', 26),\n",
       "             ('belted', 26),\n",
       "             ('round', 364),\n",
       "             ('by', 962),\n",
       "             ('wharves', 26),\n",
       "             ('indian', 52),\n",
       "             ('isles', 26),\n",
       "             ('coral', 26),\n",
       "             ('reefs', 26),\n",
       "             ('commerce', 26),\n",
       "             ('surrounds', 26),\n",
       "             ('her', 156),\n",
       "             ('surf', 26),\n",
       "             ('right', 156),\n",
       "             ('left', 78),\n",
       "             ('streets', 208),\n",
       "             ('you', 2158),\n",
       "             ('waterward', 52),\n",
       "             ('its', 156),\n",
       "             ('extreme', 26),\n",
       "             ('downtown', 26),\n",
       "             ('battery', 52),\n",
       "             ('where', 364),\n",
       "             ('noble', 52),\n",
       "             ('mole', 26),\n",
       "             ('washed', 52),\n",
       "             ('waves', 26),\n",
       "             ('cooled', 26),\n",
       "             ('breezes', 26),\n",
       "             ('which', 572),\n",
       "             ('few', 104),\n",
       "             ('hours', 130),\n",
       "             ('previous', 104),\n",
       "             ('were', 962),\n",
       "             ('out', 956),\n",
       "             ('sight', 104),\n",
       "             ('land', 208),\n",
       "             ('look', 156),\n",
       "             ('at', 2184),\n",
       "             ('crowds', 52),\n",
       "             ('water', 234),\n",
       "             ('gazers', 26),\n",
       "             ('circumambulate', 26),\n",
       "             ('dreamy', 26),\n",
       "             ('sabbath', 52),\n",
       "             ('afternoon', 52),\n",
       "             ('go', 494),\n",
       "             ('corlears', 26),\n",
       "             ('hook', 26),\n",
       "             ('coenties', 26),\n",
       "             ('slip', 26),\n",
       "             ('thence', 52),\n",
       "             ('whitehall', 26),\n",
       "             ('northward', 26),\n",
       "             ('what', 1118),\n",
       "             ('do', 702),\n",
       "             ('see?--posted', 26),\n",
       "             ('like', 732),\n",
       "             ('silent', 52),\n",
       "             ('sentinels', 26),\n",
       "             ('around', 78),\n",
       "             ('town', 156),\n",
       "             ('stand', 182),\n",
       "             ('thousands', 52),\n",
       "             ('mortal', 26),\n",
       "             ('fixed', 78),\n",
       "             ('reveries', 52),\n",
       "             ('leaning', 52),\n",
       "             ('against', 234),\n",
       "             ('spiles', 26),\n",
       "             ('seated', 52),\n",
       "             ('pier', 26),\n",
       "             ('heads', 338),\n",
       "             ('looking', 312),\n",
       "             ('over', 702),\n",
       "             ('bulwarks', 52),\n",
       "             ('ships', 78),\n",
       "             ('china', 26),\n",
       "             ('aloft', 52),\n",
       "             ('rigging', 26),\n",
       "             ('striving', 26),\n",
       "             ('still', 364),\n",
       "             ('better', 208),\n",
       "             ('seaward', 26),\n",
       "             ('peep', 26),\n",
       "             ('these', 494),\n",
       "             ('are', 416),\n",
       "             ('landsmen', 26),\n",
       "             ('week', 52),\n",
       "             ('days', 104),\n",
       "             ('pent', 26),\n",
       "             ('lath', 26),\n",
       "             ('plaster', 52),\n",
       "             ('tied', 26),\n",
       "             ('counters', 26),\n",
       "             ('nailed', 26),\n",
       "             ('benches', 26),\n",
       "             ('clinched', 26),\n",
       "             ('desks', 26),\n",
       "             ('green', 130),\n",
       "             ('fields', 26),\n",
       "             ('gone', 52),\n",
       "             ('here', 598),\n",
       "             ('come', 338),\n",
       "             ('more', 494),\n",
       "             ('pacing', 26),\n",
       "             ('straight', 104),\n",
       "             ('seemingly', 26),\n",
       "             ('bound', 52),\n",
       "             ('dive', 26),\n",
       "             ('strange', 182),\n",
       "             ('will', 260),\n",
       "             ('content', 52),\n",
       "             ('them', 442),\n",
       "             ('extremest', 26),\n",
       "             ('limit', 26),\n",
       "             ('loitering', 26),\n",
       "             ('under', 260),\n",
       "             ('shady', 26),\n",
       "             ('lee', 26),\n",
       "             ('yonder', 52),\n",
       "             ('not', 1534),\n",
       "             ('suffice', 26),\n",
       "             ('must', 442),\n",
       "             ('just', 390),\n",
       "             ('nigh', 104),\n",
       "             ('possibly', 26),\n",
       "             ('without', 182),\n",
       "             ('falling', 52),\n",
       "             ('miles', 78),\n",
       "             ('leagues', 26),\n",
       "             ('inlanders', 26),\n",
       "             ('lanes', 26),\n",
       "             ('alleys', 26),\n",
       "             ('avenues', 26),\n",
       "             ('north', 52),\n",
       "             ('east', 26),\n",
       "             ('south', 156),\n",
       "             ('west', 26),\n",
       "             ('yet', 416),\n",
       "             ('unite', 26),\n",
       "             ('tell', 442),\n",
       "             ('does', 156),\n",
       "             ('magnetic', 26),\n",
       "             ('virtue', 26),\n",
       "             ('needles', 26),\n",
       "             ('compasses', 26),\n",
       "             ('those', 234),\n",
       "             ('attract', 26),\n",
       "             ('thither', 26),\n",
       "             ('once', 208),\n",
       "             ('say', 286),\n",
       "             ('country', 78),\n",
       "             ('lakes', 26),\n",
       "             ('any', 364),\n",
       "             ('path', 26),\n",
       "             ('please', 52),\n",
       "             ('ten', 52),\n",
       "             ('one', 1300),\n",
       "             ('carries', 26),\n",
       "             ('down', 468),\n",
       "             ('dale', 26),\n",
       "             ('leaves', 52),\n",
       "             ('pool', 26),\n",
       "             ('stream', 78),\n",
       "             ('magic', 52),\n",
       "             ('let', 156),\n",
       "             ('most', 468),\n",
       "             ('absent', 26),\n",
       "             ('minded', 26),\n",
       "             ('be', 1716),\n",
       "             ('plunged', 52),\n",
       "             ('deepest', 26),\n",
       "             ('man', 572),\n",
       "             ('legs', 104),\n",
       "             ('set', 156),\n",
       "             ('feet', 182),\n",
       "             ('going', 260),\n",
       "             ('he', 3247),\n",
       "             ('infallibly', 26),\n",
       "             ('lead', 78),\n",
       "             ('region', 26),\n",
       "             ('should', 286),\n",
       "             ('ever', 338),\n",
       "             ('athirst', 26),\n",
       "             ('great', 376),\n",
       "             ('american', 78),\n",
       "             ('desert', 26),\n",
       "             ('try', 104),\n",
       "             ('experiment', 26),\n",
       "             ('caravan', 26),\n",
       "             ('happen', 26),\n",
       "             ('supplied', 26),\n",
       "             ('metaphysical', 52),\n",
       "             ('professor', 26),\n",
       "             ('yes', 104),\n",
       "             ('knows', 26),\n",
       "             ('meditation', 26),\n",
       "             ('wedded', 26),\n",
       "             ('artist', 78),\n",
       "             ('desires', 26),\n",
       "             ('paint', 26),\n",
       "             ('dreamiest', 26),\n",
       "             ('shadiest', 26),\n",
       "             ('quietest', 26),\n",
       "             ('enchanting', 26),\n",
       "             ('bit', 130),\n",
       "             ('romantic', 26),\n",
       "             ('landscape', 26),\n",
       "             ('valley', 26),\n",
       "             ('saco', 26),\n",
       "             ('chief', 52),\n",
       "             ('element', 26),\n",
       "             ('employs', 26),\n",
       "             ('trees', 26),\n",
       "             ('each', 78),\n",
       "             ('hollow', 26),\n",
       "             ('trunk', 52),\n",
       "             ('hermit', 26),\n",
       "             ('crucifix', 26),\n",
       "             ('within', 130),\n",
       "             ('sleeps', 26),\n",
       "             ('meadow', 52),\n",
       "             ('sleep', 416),\n",
       "             ('cattle', 26),\n",
       "             ('cottage', 26),\n",
       "             ('goes', 78),\n",
       "             ('sleepy', 26),\n",
       "             ('smoke', 52),\n",
       "             ('deep', 78),\n",
       "             ('distant', 78),\n",
       "             ('woodlands', 26),\n",
       "             ('winds', 78),\n",
       "             ('mazy', 26),\n",
       "             ('reaching', 52),\n",
       "             ('overlapping', 26),\n",
       "             ('spurs', 26),\n",
       "             ('mountains', 26),\n",
       "             ('bathed', 26),\n",
       "             ('hill', 52),\n",
       "             ('side', 286),\n",
       "             ('blue', 78),\n",
       "             ('though', 494),\n",
       "             ('picture', 130),\n",
       "             ('lies', 26),\n",
       "             ('thus', 52),\n",
       "             ('tranced', 26),\n",
       "             ('pine', 52),\n",
       "             ('tree', 26),\n",
       "             ('shakes', 26),\n",
       "             ('sighs', 52),\n",
       "             ('shepherd', 52),\n",
       "             ('head', 598),\n",
       "             ('vain', 26),\n",
       "             ('unless', 104),\n",
       "             ('eye', 26),\n",
       "             ('him', 1092),\n",
       "             ('visit', 26),\n",
       "             ('prairies', 26),\n",
       "             ('june', 52),\n",
       "             ('when', 650),\n",
       "             ('scores', 52),\n",
       "             ('wade', 26),\n",
       "             ('knee', 26),\n",
       "             ('among', 78),\n",
       "             ('tiger', 26),\n",
       "             ('lilies', 26),\n",
       "             ('charm', 26),\n",
       "             ('wanting?--water', 26),\n",
       "             ('drop', 26),\n",
       "             ('niagara', 26),\n",
       "             ('cataract', 26),\n",
       "             ('sand', 26),\n",
       "             ('travel', 26),\n",
       "             ('thousand', 52),\n",
       "             ('why', 286),\n",
       "             ('did', 572),\n",
       "             ('poor', 104),\n",
       "             ('poet', 26),\n",
       "             ('tennessee', 26),\n",
       "             ('suddenly', 78),\n",
       "             ('receiving', 26),\n",
       "             ('two', 338),\n",
       "             ('handfuls', 26),\n",
       "             ('silver', 26),\n",
       "             ('deliberate', 26),\n",
       "             ('whether', 182),\n",
       "             ('buy', 26),\n",
       "             ('coat', 104),\n",
       "             ('sadly', 52),\n",
       "             ('needed', 26),\n",
       "             ('invest', 26),\n",
       "             ('pedestrian', 26),\n",
       "             ('trip', 26),\n",
       "             ('rockaway', 26),\n",
       "             ('beach', 26),\n",
       "             ('robust', 52),\n",
       "             ('healthy', 52),\n",
       "             ('boy', 52),\n",
       "             ('crazy', 52),\n",
       "             ('first', 494),\n",
       "             ('voyage', 208),\n",
       "             ('passenger', 104),\n",
       "             ('yourself', 156),\n",
       "             ('feel', 78),\n",
       "             ('mystical', 26),\n",
       "             ('vibration', 26),\n",
       "             ('told', 130),\n",
       "             ('old', 754),\n",
       "             ('persians', 26),\n",
       "             ('hold', 52),\n",
       "             ('holy', 52),\n",
       "             ('greeks', 26),\n",
       "             ('give', 208),\n",
       "             ('separate', 26),\n",
       "             ('deity', 26),\n",
       "             ('own', 286),\n",
       "             ('brother', 52),\n",
       "             ('jove', 26),\n",
       "             ('surely', 26),\n",
       "             ('meaning', 78),\n",
       "             ('deeper', 26),\n",
       "             ('story', 130),\n",
       "             ('narcissus', 26),\n",
       "             ('who', 416),\n",
       "             ('because', 182),\n",
       "             ('could', 650),\n",
       "             ('grasp', 26),\n",
       "             ('tormenting', 26),\n",
       "             ('mild', 26),\n",
       "             ('image', 156),\n",
       "             ('saw', 156),\n",
       "             ('fountain', 26),\n",
       "             ('was', 2886),\n",
       "             ('drowned', 26),\n",
       "             ('we', 286),\n",
       "             ('ourselves', 52),\n",
       "             ('rivers', 26),\n",
       "             ('oceans', 26),\n",
       "             ('ungraspable', 26),\n",
       "             ('phantom', 78),\n",
       "             ('life', 78),\n",
       "             ('key', 26),\n",
       "             ('am', 156),\n",
       "             ('habit', 26),\n",
       "             ('begin', 52),\n",
       "             ('grow', 52),\n",
       "             ('hazy', 26),\n",
       "             ('eyes', 182),\n",
       "             ('conscious', 26),\n",
       "             ('lungs', 26),\n",
       "             ('mean', 130),\n",
       "             ('inferred', 52),\n",
       "             ('needs', 52),\n",
       "             ('rag', 26),\n",
       "             ('something', 312),\n",
       "             ('besides', 156),\n",
       "             ('passengers', 78),\n",
       "             ('sick', 26),\n",
       "             ('quarrelsome', 26),\n",
       "             (\"don't\", 52),\n",
       "             ('nights', 26),\n",
       "             ('enjoy', 26),\n",
       "             ('themselves', 52),\n",
       "             ('much', 442),\n",
       "             ('general', 26),\n",
       "             ('thing;--no', 26),\n",
       "             ('nor', 78),\n",
       "             ('salt', 26),\n",
       "             ('commodore', 52),\n",
       "             ('captain', 52),\n",
       "             ('cook', 52),\n",
       "             ('abandon', 26),\n",
       "             ('glory', 52),\n",
       "             ('distinction', 26),\n",
       "             ('offices', 26),\n",
       "             ('abominate', 26),\n",
       "             ('honourable', 26),\n",
       "             ('respectable', 26),\n",
       "             ('toils', 26),\n",
       "             ('trials', 26),\n",
       "             ('tribulations', 26),\n",
       "             ('kind', 78),\n",
       "             ('whatsoever', 52),\n",
       "             ('quite', 78),\n",
       "             ('care', 78),\n",
       "             ('taking', 104),\n",
       "             ('barques', 26),\n",
       "             ('brigs', 26),\n",
       "             ('schooners', 26),\n",
       "             ('cook,--though', 26),\n",
       "             ('confess', 52),\n",
       "             ('considerable', 26),\n",
       "             ('being', 390),\n",
       "             ('sort', 494),\n",
       "             ('officer', 52),\n",
       "             ('board', 78),\n",
       "             ('somehow', 78),\n",
       "             ('fancied', 26),\n",
       "             ('broiling', 26),\n",
       "             ('fowls;--though', 26),\n",
       "             ('broiled', 78),\n",
       "             ('judiciously', 26),\n",
       "             ('buttered', 26),\n",
       "             ('judgmatically', 26),\n",
       "             ('salted', 26),\n",
       "             ('peppered', 26),\n",
       "             ('speak', 130),\n",
       "             ('respectfully', 52),\n",
       "             ('reverentially', 26),\n",
       "             ('fowl', 26),\n",
       "             ('than', 390),\n",
       "             ('idolatrous', 26),\n",
       "             ('dotings', 26),\n",
       "             ('egyptians', 26),\n",
       "             ('ibis', 26),\n",
       "             ('roasted', 26),\n",
       "             ('river', 26),\n",
       "             ('horse', 52),\n",
       "             ('mummies', 26),\n",
       "             ('creatures', 26),\n",
       "             ('huge', 52),\n",
       "             ('bake', 26),\n",
       "             ('houses', 52),\n",
       "             ('pyramids', 26),\n",
       "             ('simple', 26),\n",
       "             ('sailor', 156),\n",
       "             ('mast', 78),\n",
       "             ('plumb', 26),\n",
       "             ('forecastle', 52),\n",
       "             ('royal', 26),\n",
       "             ('true', 104),\n",
       "             ('rather', 208),\n",
       "             ('order', 130),\n",
       "             ('make', 260),\n",
       "             ('jump', 52),\n",
       "             ('spar', 52),\n",
       "             ('grasshopper', 26),\n",
       "             ('may', 312),\n",
       "             ('thing', 104),\n",
       "             ('unpleasant', 26),\n",
       "             ('enough', 338),\n",
       "             ('touches', 26),\n",
       "             ('sense', 78),\n",
       "             ('honour', 26),\n",
       "             ('particularly', 52),\n",
       "             ('established', 26),\n",
       "             ('family', 26),\n",
       "             ('van', 26),\n",
       "             ('rensselaers', 26),\n",
       "             ('randolphs', 26),\n",
       "             ('hardicanutes', 26),\n",
       "             ('putting', 52),\n",
       "             ('tar', 52),\n",
       "             ('pot', 26),\n",
       "             ('been', 468),\n",
       "             ('lording', 26),\n",
       "             ('schoolmaster', 52),\n",
       "             ('making', 130),\n",
       "             ('tallest', 26),\n",
       "             ('boys', 52),\n",
       "             ('awe', 26),\n",
       "             ('transition', 52),\n",
       "             ('keen', 26),\n",
       "             ('assure', 26),\n",
       "             ('decoction', 26),\n",
       "             ('seneca', 26),\n",
       "             ('stoics', 26),\n",
       "             ('enable', 26),\n",
       "             ('grin', 52),\n",
       "             ('bear', 52),\n",
       "             ('even', 130),\n",
       "             ('wears', 26),\n",
       "             ('hunks', 52),\n",
       "             ('orders', 26),\n",
       "             ('broom', 26),\n",
       "             ('sweep', 52),\n",
       "             ('decks', 26),\n",
       "             ('indignity', 26),\n",
       "             ('amount', 26),\n",
       "             ('weighed', 52),\n",
       "             ('scales', 26),\n",
       "             ('new', 286),\n",
       "             ('testament', 26),\n",
       "             ('think', 182),\n",
       "             ('archangel', 26),\n",
       "             ('gabriel', 26),\n",
       "             ('thinks', 182),\n",
       "             ('anything', 52),\n",
       "             ('less', 52),\n",
       "             ('promptly', 26),\n",
       "             ('obey', 26),\n",
       "             ('instance', 26),\n",
       "             ('ai', 104),\n",
       "             (\"n't\", 624),\n",
       "             ('slave', 26),\n",
       "             ('well', 208),\n",
       "             ('however', 208),\n",
       "             ('captains', 26),\n",
       "             ('thump', 52),\n",
       "             ('punch', 26),\n",
       "             ('satisfaction', 26),\n",
       "             ('knowing', 78),\n",
       "             ('everybody', 26),\n",
       "             ('else', 208),\n",
       "             ('served', 26),\n",
       "             ('either', 78),\n",
       "             ('physical', 26),\n",
       "             ('point', 52),\n",
       "             ('view', 52),\n",
       "             ('so', 1066),\n",
       "             ('universal', 26),\n",
       "             ('passed', 78),\n",
       "             ('hands', 78),\n",
       "             ('rub', 26),\n",
       "             ('shoulder', 26),\n",
       "             ('blades', 26),\n",
       "             ('again', 286),\n",
       "             ('always', 104),\n",
       "             ('paying', 78),\n",
       "             ('trouble', 26),\n",
       "             ('whereas', 26),\n",
       "             ('pay', 78),\n",
       "             ('single', 52),\n",
       "             ('penny', 78),\n",
       "             ('heard', 208),\n",
       "             ('contrary', 26),\n",
       "             ('difference', 52),\n",
       "             ('between', 234),\n",
       "             ('paid', 26),\n",
       "             ('act', 52),\n",
       "             ('perhaps', 130),\n",
       "             ('uncomfortable', 52),\n",
       "             ('infliction', 26),\n",
       "             ('orchard', 26),\n",
       "             ('thieves', 26),\n",
       "             ('entailed', 26),\n",
       "             ('us', 104),\n",
       "             ('paid,--what', 26),\n",
       "             ('compare', 52),\n",
       "             ('urbane', 26),\n",
       "             ('activity', 26),\n",
       "             ('receives', 26),\n",
       "             ('really', 104),\n",
       "             ('marvellous', 104),\n",
       "             ('considering', 26),\n",
       "             ('earnestly', 26),\n",
       "             ('believe', 26),\n",
       "             ('root', 26),\n",
       "             ('earthly', 52),\n",
       "             ('ills', 26),\n",
       "             ('monied', 26),\n",
       "             ('enter', 52),\n",
       "             ('heaven', 104),\n",
       "             ('ah', 26),\n",
       "             ('cheerfully', 26),\n",
       "             ('consign', 26),\n",
       "             ('perdition', 26),\n",
       "             ('finally', 26),\n",
       "             ('wholesome', 26),\n",
       "             ('exercise', 26),\n",
       "             ('pure', 26),\n",
       "             ('air', 104),\n",
       "             ('fore', 26),\n",
       "             ('castle', 26),\n",
       "             ('deck', 52),\n",
       "             ('far', 104),\n",
       "             ('prevalent', 26),\n",
       "             ('astern', 26),\n",
       "             ('violate', 26),\n",
       "             ('pythagorean', 26),\n",
       "             ('maxim', 26),\n",
       "             ('quarter', 52),\n",
       "             ('gets', 26),\n",
       "             ('atmosphere', 26),\n",
       "             ('second', 104),\n",
       "             ('sailors', 78),\n",
       "             ('breathes', 26),\n",
       "             ('commonalty', 26),\n",
       "             ('leaders', 52),\n",
       "             ('many', 104),\n",
       "             ('things', 130),\n",
       "             ('suspect', 26),\n",
       "             ('wherefore', 26),\n",
       "             ('after', 234),\n",
       "             ('repeatedly', 26),\n",
       "             ('smelt', 52),\n",
       "             ('merchant', 26),\n",
       "             ('whaling', 234),\n",
       "             ('invisible', 26),\n",
       "             ('police', 26),\n",
       "             ('fates', 52),\n",
       "             ('has', 104),\n",
       "             ('constant', 26),\n",
       "             ('surveillance', 26),\n",
       "             ('secretly', 26),\n",
       "             ('dogs', 26),\n",
       "             ('influences', 26),\n",
       "             ('unaccountable', 104),\n",
       "             ('answer', 130),\n",
       "             ('doubtless', 52),\n",
       "             ('formed', 52),\n",
       "             ('grand', 104),\n",
       "             ('programme', 26),\n",
       "             ('providence', 26),\n",
       "             ('drawn', 26),\n",
       "             ('came', 286),\n",
       "             ('brief', 26),\n",
       "             ('interlude', 26),\n",
       "             ('solo', 26),\n",
       "             ('extensive', 26),\n",
       "             ('performances', 26),\n",
       "             ('bill', 26),\n",
       "             ('run', 52),\n",
       "             ('contested', 26),\n",
       "             ('election', 26),\n",
       "             ('presidency', 26),\n",
       "             ('united', 26),\n",
       "             ('states', 26),\n",
       "             ('bloody', 26),\n",
       "             ('battle', 26),\n",
       "             ('affghanistan', 26),\n",
       "             ('exactly', 78),\n",
       "             ('stage', 52),\n",
       "             ('managers', 26),\n",
       "             ('put', 208),\n",
       "             ('shabby', 52),\n",
       "             ('others', 52),\n",
       "             ('magnificent', 26),\n",
       "             ('parts', 130),\n",
       "             ('tragedies', 26),\n",
       "             ('short', 78),\n",
       "             ('easy', 78),\n",
       "             ('genteel', 26),\n",
       "             ('comedies', 26),\n",
       "             ('jolly', 104),\n",
       "             ('farces', 26),\n",
       "             ('recall', 26),\n",
       "             ('circumstances', 52),\n",
       "             ('springs', 26),\n",
       "             ('motives', 52),\n",
       "             ('cunningly', 26),\n",
       "             ('presented', 26),\n",
       "             ('various', 52),\n",
       "             ('disguises', 26),\n",
       "             ('induced', 26),\n",
       "             ('performing', 26),\n",
       "             ('cajoling', 26),\n",
       "             ('delusion', 26),\n",
       "             ('choice', 26),\n",
       "             ('resulting', 26),\n",
       "             ('unbiased', 26),\n",
       "             ('freewill', 26),\n",
       "             ('discriminating', 26),\n",
       "             ('judgment', 26),\n",
       "             ('overwhelming', 26),\n",
       "             ('idea', 182),\n",
       "             ('whale', 260),\n",
       "             ('portentous', 78),\n",
       "             ('mysterious', 52),\n",
       "             ('monster', 26),\n",
       "             ('roused', 26),\n",
       "             ('curiosity', 52),\n",
       "             ('wild', 130),\n",
       "             ('seas', 156),\n",
       "             ('rolled', 156),\n",
       "             ('island', 78),\n",
       "             ('bulk', 26),\n",
       "             ('undeliverable', 26),\n",
       "             ('nameless', 78),\n",
       "             ('perils', 26),\n",
       "             ('attending', 26),\n",
       "             ('marvels', 26),\n",
       "             ('patagonian', 26),\n",
       "             ('sights', 26),\n",
       "             ('sounds', 78),\n",
       "             ('helped', 26),\n",
       "             ('sway', 26),\n",
       "             ('wish', 26),\n",
       "             ('inducements', 26),\n",
       "             ('tormented', 52),\n",
       "             ('everlasting', 52),\n",
       "             ('itch', 26),\n",
       "             ('remote', 26),\n",
       "             ('love', 26),\n",
       "             ('forbidden', 26),\n",
       "             ('barbarous', 26),\n",
       "             ('coasts', 26),\n",
       "             ('ignoring', 26),\n",
       "             ('good', 442),\n",
       "             ('quick', 26),\n",
       "             ('perceive', 26),\n",
       "             ('horror', 26),\n",
       "             ('social', 26),\n",
       "             ('since', 78),\n",
       "             ('friendly', 26),\n",
       "             ('terms', 26),\n",
       "             ('inmates', 26),\n",
       "             ('place', 390),\n",
       "             ('lodges', 26),\n",
       "             ('reason', 130),\n",
       "             ('welcome', 26),\n",
       "             ('flood', 26),\n",
       "             ('gates', 26),\n",
       "             ('wonder', 52),\n",
       "             ('swung', 26),\n",
       "             ('open', 104),\n",
       "             ('conceits', 26),\n",
       "             ('swayed', 26),\n",
       "             ('purpose', 52),\n",
       "             ('floated', 52),\n",
       "             ('inmost', 26),\n",
       "             ('endless', 26),\n",
       "             ('processions', 26),\n",
       "             ('mid', 26),\n",
       "             ('hooded', 26),\n",
       "             ('snow', 78),\n",
       "             ('stuffed', 52),\n",
       "             ('shirt', 104),\n",
       "             ('carpet', 26),\n",
       "             ('bag', 182),\n",
       "             ('tucked', 26),\n",
       "             ('arm', 286),\n",
       "             ('started', 26),\n",
       "             ('cape', 104),\n",
       "             ('horn', 52),\n",
       "             ('pacific', 26),\n",
       "             ('quitting', 26),\n",
       "             ('manhatto', 26),\n",
       "             ('duly', 26),\n",
       "             ('arrived', 52),\n",
       "             ('bedford', 104),\n",
       "             ('saturday', 78),\n",
       "             ('night', 624),\n",
       "             ('december', 26),\n",
       "             ('disappointed', 26),\n",
       "             ('learning', 26),\n",
       "             ('packet', 26),\n",
       "             ('nantucket', 182),\n",
       "             ('had', 858),\n",
       "             ('already', 26),\n",
       "             ('sailed', 26),\n",
       "             ('offer', 52),\n",
       "             ('till', 156),\n",
       "             ('following', 52),\n",
       "             ('monday', 26),\n",
       "             ('young', 130),\n",
       "             ('candidates', 26),\n",
       "             ('pains', 26),\n",
       "             ('penalties', 26),\n",
       "             ('stop', 208),\n",
       "             ('embark', 52),\n",
       "             ('related', 26),\n",
       "             ('doing', 26),\n",
       "             ('made', 338),\n",
       "             ('craft', 130),\n",
       "             ('fine', 104),\n",
       "             ('boisterous', 26),\n",
       "             ('everything', 26),\n",
       "             ('connected', 26),\n",
       "             ('famous', 26),\n",
       "             ('amazingly', 26),\n",
       "             ('pleased', 52),\n",
       "             ('late', 182),\n",
       "             ('gradually', 26),\n",
       "             ('monopolising', 26),\n",
       "             ('business', 130),\n",
       "             ('matter', 78),\n",
       "             ('behind', 26),\n",
       "             ('original', 52),\n",
       "             ('tyre', 26),\n",
       "             ('carthage;--the', 26),\n",
       "             ('dead', 130),\n",
       "             ('stranded', 52),\n",
       "             ('aboriginal', 26),\n",
       "             ('whalemen', 26),\n",
       "             ('red', 78),\n",
       "             ('sally', 26),\n",
       "             ('canoes', 26),\n",
       "             ('chase', 26),\n",
       "             ('leviathan', 52),\n",
       "             ('too', 364),\n",
       "             ('adventurous', 26),\n",
       "             ('sloop', 26),\n",
       "             ('forth', 26),\n",
       "             ('partly', 78),\n",
       "             ('laden', 26),\n",
       "             ('imported', 26),\n",
       "             ('cobblestones', 26),\n",
       "             ('throw', 26),\n",
       "             ('whales', 52),\n",
       "             ('discover', 26),\n",
       "             ('risk', 26),\n",
       "             ('harpoon', 135),\n",
       "             ('bowsprit', 26),\n",
       "             ('day', 156),\n",
       "             ('another', 130),\n",
       "             ('ere', 78),\n",
       "             ('destined', 26),\n",
       "             ('port', 26),\n",
       "             ('became', 78),\n",
       "             ('concernment', 26),\n",
       "             ('eat', 26),\n",
       "             ('meanwhile', 78),\n",
       "             ('dubious', 26),\n",
       "             ('nay', 52),\n",
       "             ('dark', 208),\n",
       "             ('dismal', 52),\n",
       "             ('bitingly', 26),\n",
       "             ('cold', 182),\n",
       "             ('cheerless', 26),\n",
       "             ('anxious', 26),\n",
       "             ('grapnels', 26),\n",
       "             ('sounded', 26),\n",
       "             ('pocket', 78),\n",
       "             ('only', 364),\n",
       "             ('brought', 26),\n",
       "             ('pieces', 26),\n",
       "             ('silver,--so', 26),\n",
       "             ('wherever', 52),\n",
       "             ('said', 494),\n",
       "             ('stood', 260),\n",
       "             ('middle', 130),\n",
       "             ('dreary', 52),\n",
       "             ('shouldering', 26),\n",
       "             ('comparing', 26),\n",
       "             ('gloom', 26),\n",
       "             ('darkness', 78),\n",
       "             ('wisdom', 26),\n",
       "             ('conclude', 26),\n",
       "             ('lodge', 26),\n",
       "             ('dear', 26),\n",
       "             ('sure', 130),\n",
       "             ('inquire', 26),\n",
       "             ('price', 26),\n",
       "             ('halting', 26),\n",
       "             ('steps', 26),\n",
       "             ('paced', 26),\n",
       "             ('sign', 156),\n",
       "             ('crossed', 52),\n",
       "             ('harpoons\"--but', 26),\n",
       "             ('looked', 156),\n",
       "             ('expensive', 52),\n",
       "             ('further', 104),\n",
       "             ('bright', 52),\n",
       "             ('windows', 52),\n",
       "             ('fish', 78),\n",
       "             ('inn', 78),\n",
       "             ('fervent', 26),\n",
       "             ('rays', 26),\n",
       "             ('seemed', 416),\n",
       "             ('melted', 26),\n",
       "             ('packed', 52),\n",
       "             ('ice', 104),\n",
       "             ('house', 286),\n",
       "             ('everywhere', 26),\n",
       "             ('congealed', 26),\n",
       "             ('frost', 104),\n",
       "             ('lay', 234),\n",
       "             ('inches', 52),\n",
       "             ('thick', 52),\n",
       "             ...])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Numpy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)    # listeyi array dönüştürmek gerekiyor işlem yapabilmek için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 26)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11338\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in tokens:\n",
    "    a = a + 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11312"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11338-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>956</td>\n",
       "      <td>14</td>\n",
       "      <td>263</td>\n",
       "      <td>51</td>\n",
       "      <td>261</td>\n",
       "      <td>408</td>\n",
       "      <td>87</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>2713</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>263</td>\n",
       "      <td>51</td>\n",
       "      <td>261</td>\n",
       "      <td>408</td>\n",
       "      <td>87</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>954</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>2713</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263</td>\n",
       "      <td>51</td>\n",
       "      <td>261</td>\n",
       "      <td>408</td>\n",
       "      <td>87</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>954</td>\n",
       "      <td>260</td>\n",
       "      <td>...</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>2713</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>957</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>261</td>\n",
       "      <td>408</td>\n",
       "      <td>87</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>954</td>\n",
       "      <td>260</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>2713</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>957</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261</td>\n",
       "      <td>408</td>\n",
       "      <td>87</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>111</td>\n",
       "      <td>954</td>\n",
       "      <td>260</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>2713</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>957</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11307</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>952</td>\n",
       "      <td>12</td>\n",
       "      <td>166</td>\n",
       "      <td>2712</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2714</td>\n",
       "      <td>2715</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>2716</td>\n",
       "      <td>547</td>\n",
       "      <td>955</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>11</td>\n",
       "      <td>952</td>\n",
       "      <td>12</td>\n",
       "      <td>166</td>\n",
       "      <td>2712</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2714</td>\n",
       "      <td>2715</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>2716</td>\n",
       "      <td>547</td>\n",
       "      <td>955</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>952</td>\n",
       "      <td>12</td>\n",
       "      <td>166</td>\n",
       "      <td>2712</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2714</td>\n",
       "      <td>2715</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>2716</td>\n",
       "      <td>547</td>\n",
       "      <td>955</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>12</td>\n",
       "      <td>166</td>\n",
       "      <td>2712</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2714</td>\n",
       "      <td>2715</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2716</td>\n",
       "      <td>547</td>\n",
       "      <td>955</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>166</td>\n",
       "      <td>2712</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2714</td>\n",
       "      <td>2715</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>547</td>\n",
       "      <td>955</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2718</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11312 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9   ...    16  \\\n",
       "0      956    14   263    51   261   408    87   219   129   111  ...     7   \n",
       "1       14   263    51   261   408    87   219   129   111   954  ...    23   \n",
       "2      263    51   261   408    87   219   129   111   954   260  ...   546   \n",
       "3       51   261   408    87   219   129   111   954   260    50  ...     3   \n",
       "4      261   408    87   219   129   111   954   260    50    43  ...   150   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "11307    4    11   952    12   166  2712     3    10  2714  2715  ...     7   \n",
       "11308   11   952    12   166  2712     3    10  2714  2715    42  ...    11   \n",
       "11309  952    12   166  2712     3    10  2714  2715    42     4  ...   110   \n",
       "11310   12   166  2712     3    10  2714  2715    42     4     1  ...  2716   \n",
       "11311  166  2712     3    10  2714  2715    42     4     1    61  ...   547   \n",
       "\n",
       "         17    18    19    20    21    22    23    24    25  \n",
       "0        23   546     3   150   259     6  2713    14    24  \n",
       "1       546     3   150   259     6  2713    14    24   957  \n",
       "2         3   150   259     6  2713    14    24   957     5  \n",
       "3       150   259     6  2713    14    24   957     5    60  \n",
       "4       259     6  2713    14    24   957     5    60     5  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "11307    11   110  2716   547   955     3  2717    11   262  \n",
       "11308   110  2716   547   955     3  2717    11   262    53  \n",
       "11309  2716   547   955     3  2717    11   262    53     2  \n",
       "11310   547   955     3  2717    11   262    53     2  2718  \n",
       "11311   955     3  2717    11   262    53     2  2718    26  \n",
       "\n",
       "[11312 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deneme = pd.DataFrame(sequences)\n",
    "df_deneme\n",
    "\n",
    "# 26. kolon bizim target'ımız"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an LSTM based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):           # CHATGPT daha detaylı yazdı\n",
    "    # Modeli başlat\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1. Embedding Katmanı\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=25, input_length=seq_len)) \n",
    "    # input_dim: Kelime dağarcığı boyutu (2717)\n",
    "    # output_dim: Kelimelerin vektör boyutu (25)\n",
    "    # input_length: Giriş dizisinin uzunluğu (25)\n",
    "\n",
    "    # 2. İlk LSTM Katmanı\n",
    "    model.add(LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # 150 nöronlu LSTM, her zaman diliminde bir çıktı döndürür\n",
    "    # dropout: Aşırı öğrenmeyi önlemek için nöronları rasgele kapatır\n",
    "    # recurrent_dropout: LSTM'nin içsel geri bağlantılarını kullanırken dropout uygular\n",
    "\n",
    "    # 3. İkinci LSTM Katmanı\n",
    "    model.add(LSTM(150, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # Bu katman, sadece son zaman dilimi için bir çıktı döndürür\n",
    "\n",
    "    # 4. Dropout Katmanı\n",
    "    model.add(Dropout(0.3))\n",
    "    # Aşırı öğrenmeyi önlemek için ek bir dropout katmanı\n",
    "\n",
    "    # 5. Dense Katmanı (Ara Katman)\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "    # Ara katman, nöron sayısını 150 olarak ayarlar ve ReLU aktivasyon fonksiyonu kullanır\n",
    "\n",
    "    # 6. Çıkış Katmanı\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    # Kelime tahmini için softmax aktivasyon fonksiyonu ile çıkış katmanı\n",
    "\n",
    "    # Modelin derlenmesi\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Model özeti\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss='categorical_crossentropy': Çok sınıflı sınıflandırma problemleri için uygun olan kayıp fonksiyonudur.\n",
    "\n",
    "- softmax aktivasyon fonksiyonu : her bir çıkış biriminin 0 ile 1 arasında bir değer almasını ve tüm çıkışların toplamının 1 olmasını sağlar. Bu, modelin her kelime için bir olasılık dağılımı tahmin etmesini sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ...,    6, 2713,   14],\n",
       "       [  14,  263,   51, ..., 2713,   14,   24],\n",
       "       [ 263,   51,  261, ...,   14,   24,  957],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,   11,  262,   53],\n",
       "       [  12,  166, 2712, ...,  262,   53,    2],\n",
       "       [ 166, 2712,    3, ...,   53,    2, 2718]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 49 words\n",
    "sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 2719)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to_categorical: Keras kütüphanesinin bir fonksiyonu olup, y dizisini one-hot encoding formatına dönüştürür.\n",
    "\n",
    "- y: Modelin hedef değişkeni, yani tahmin edilmek istenen sınıfları içeren dizi.\n",
    "\n",
    "- num_classes=vocabulary_size+1: One-hot encoding işleminde toplam sınıf sayısını belirtir. Eğer vocabulary_size, modelin öğrenmesini istediğimiz kelime hazinesinin boyutunu temsil ediyorsa, bu değere +1 eklenmesinin nedeni genellikle boş bir sınıf (veya yer tutucu) eklemektir. Bu, modelin 0 indeksli sınıfın (genellikle \"no class\" veya \"no prediction\" gibi) olabileceğini varsayar.\n",
    "\n",
    "Neden Kullanıyoruz?\n",
    "\n",
    "- Model Eğitimi: Sınıflandırma problemlerinde model, bir sınıfı diğerlerinden ayırabilmesi için her bir sınıfı ayrı bir çıktı birimi ile temsil etmelidir. One-hot encoding, bu ayrımı sağlar.\n",
    "\n",
    "- Hesaplama Kolaylığı: Kayıp fonksiyonları, özellikle çok sınıflı sınıflandırma için (örneğin, categorical_crossentropy), one-hot encoded verilerle daha etkili bir şekilde çalışır.\n",
    "\n",
    "- Model Performansı: One-hot encoding, modelin öğrenme sürecini hızlandırabilir ve performansı artırabilir, çünkü model her bir sınıfın varlığını net bir şekilde anlamış olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = X.shape[1]\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 66ms/step - accuracy: 0.0329 - loss: 7.1908\n",
      "Epoch 2/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.0527 - loss: 6.3583\n",
      "Epoch 3/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.0547 - loss: 6.2666\n",
      "Epoch 4/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.0515 - loss: 6.1446\n",
      "Epoch 5/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.0552 - loss: 6.0189\n",
      "Epoch 6/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.0640 - loss: 5.8838\n",
      "Epoch 7/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.0682 - loss: 5.7822\n",
      "Epoch 8/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.0669 - loss: 5.7040\n",
      "Epoch 9/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.0695 - loss: 5.6427\n",
      "Epoch 10/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.0712 - loss: 5.5591\n",
      "Epoch 11/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.0715 - loss: 5.5408\n",
      "Epoch 12/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.0740 - loss: 5.4720\n",
      "Epoch 13/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.0810 - loss: 5.3691\n",
      "Epoch 14/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.0837 - loss: 5.3438\n",
      "Epoch 15/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - accuracy: 0.0857 - loss: 5.2822\n",
      "Epoch 16/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - accuracy: 0.0859 - loss: 5.2475\n",
      "Epoch 17/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.0855 - loss: 5.1647\n",
      "Epoch 18/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.0876 - loss: 5.1379\n",
      "Epoch 19/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.0869 - loss: 5.0758\n",
      "Epoch 20/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.0898 - loss: 5.0463\n",
      "Epoch 21/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.0907 - loss: 5.0090\n",
      "Epoch 22/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.0917 - loss: 4.9470\n",
      "Epoch 23/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.0908 - loss: 4.9074\n",
      "Epoch 24/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.0911 - loss: 4.8545\n",
      "Epoch 25/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.0934 - loss: 4.8303\n",
      "Epoch 26/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.0956 - loss: 4.7556\n",
      "Epoch 27/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.0955 - loss: 4.7297\n",
      "Epoch 28/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.0959 - loss: 4.6642\n",
      "Epoch 29/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.0977 - loss: 4.6225\n",
      "Epoch 30/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.0933 - loss: 4.5972\n",
      "Epoch 31/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1038 - loss: 4.5387\n",
      "Epoch 32/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.1022 - loss: 4.4869\n",
      "Epoch 33/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1034 - loss: 4.4552\n",
      "Epoch 34/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1092 - loss: 4.3898\n",
      "Epoch 35/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.1099 - loss: 4.3587\n",
      "Epoch 36/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1105 - loss: 4.3086\n",
      "Epoch 37/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.1151 - loss: 4.2376\n",
      "Epoch 38/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1166 - loss: 4.2215\n",
      "Epoch 39/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.1213 - loss: 4.1481\n",
      "Epoch 40/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1217 - loss: 4.1230\n",
      "Epoch 41/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1254 - loss: 4.1042\n",
      "Epoch 42/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1319 - loss: 4.0495\n",
      "Epoch 43/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1378 - loss: 4.0272\n",
      "Epoch 44/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1369 - loss: 3.9817\n",
      "Epoch 45/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1395 - loss: 3.9314\n",
      "Epoch 46/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1437 - loss: 3.9045\n",
      "Epoch 47/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1560 - loss: 3.8440\n",
      "Epoch 48/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1535 - loss: 3.8358\n",
      "Epoch 49/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.1520 - loss: 3.8051\n",
      "Epoch 50/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.1632 - loss: 3.7511\n",
      "Epoch 51/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1713 - loss: 3.7279\n",
      "Epoch 52/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1698 - loss: 3.6971\n",
      "Epoch 53/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.1737 - loss: 3.6631\n",
      "Epoch 54/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1745 - loss: 3.6462\n",
      "Epoch 55/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1788 - loss: 3.6023\n",
      "Epoch 56/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1905 - loss: 3.5629\n",
      "Epoch 57/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1886 - loss: 3.5275\n",
      "Epoch 58/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.1959 - loss: 3.5273\n",
      "Epoch 59/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.1930 - loss: 3.4895\n",
      "Epoch 60/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.2038 - loss: 3.4658\n",
      "Epoch 61/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.2098 - loss: 3.4190\n",
      "Epoch 62/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2099 - loss: 3.4086\n",
      "Epoch 63/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2191 - loss: 3.3705\n",
      "Epoch 64/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2191 - loss: 3.3298\n",
      "Epoch 65/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2309 - loss: 3.3028\n",
      "Epoch 66/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2276 - loss: 3.3000\n",
      "Epoch 67/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2309 - loss: 3.2776\n",
      "Epoch 68/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2376 - loss: 3.2456\n",
      "Epoch 69/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2442 - loss: 3.2032\n",
      "Epoch 70/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2392 - loss: 3.1852\n",
      "Epoch 71/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2529 - loss: 3.1636\n",
      "Epoch 72/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.2565 - loss: 3.1303\n",
      "Epoch 73/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2531 - loss: 3.1239\n",
      "Epoch 74/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2665 - loss: 3.0969\n",
      "Epoch 75/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2677 - loss: 3.0621\n",
      "Epoch 76/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2635 - loss: 3.0706\n",
      "Epoch 77/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2745 - loss: 3.0315\n",
      "Epoch 78/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.2851 - loss: 3.0015\n",
      "Epoch 79/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2835 - loss: 2.9922\n",
      "Epoch 80/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.2863 - loss: 2.9634\n",
      "Epoch 81/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.2939 - loss: 2.9393\n",
      "Epoch 82/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3028 - loss: 2.9071\n",
      "Epoch 83/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3020 - loss: 2.9038\n",
      "Epoch 84/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.3053 - loss: 2.8577\n",
      "Epoch 85/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3113 - loss: 2.8639\n",
      "Epoch 86/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3052 - loss: 2.8549\n",
      "Epoch 87/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3148 - loss: 2.8117\n",
      "Epoch 88/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3136 - loss: 2.8106\n",
      "Epoch 89/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3247 - loss: 2.7843\n",
      "Epoch 90/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3333 - loss: 2.7450\n",
      "Epoch 91/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3191 - loss: 2.7546\n",
      "Epoch 92/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3396 - loss: 2.7271\n",
      "Epoch 93/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3351 - loss: 2.6915\n",
      "Epoch 94/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.3453 - loss: 2.6854\n",
      "Epoch 95/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.3384 - loss: 2.6715\n",
      "Epoch 96/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3428 - loss: 2.6685\n",
      "Epoch 97/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3434 - loss: 2.6521\n",
      "Epoch 98/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3473 - loss: 2.6392\n",
      "Epoch 99/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3467 - loss: 2.6272\n",
      "Epoch 100/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3630 - loss: 2.5682\n",
      "Epoch 101/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3704 - loss: 2.5522\n",
      "Epoch 102/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3589 - loss: 2.5702\n",
      "Epoch 103/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3685 - loss: 2.5449\n",
      "Epoch 104/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3775 - loss: 2.5262\n",
      "Epoch 105/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3760 - loss: 2.5064\n",
      "Epoch 106/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.3734 - loss: 2.5053\n",
      "Epoch 107/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3710 - loss: 2.4968\n",
      "Epoch 108/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3876 - loss: 2.4672\n",
      "Epoch 109/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3859 - loss: 2.4615\n",
      "Epoch 110/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3884 - loss: 2.4545\n",
      "Epoch 111/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3986 - loss: 2.4198\n",
      "Epoch 112/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3927 - loss: 2.4281\n",
      "Epoch 113/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4021 - loss: 2.3769\n",
      "Epoch 114/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4066 - loss: 2.3793\n",
      "Epoch 115/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3945 - loss: 2.4819\n",
      "Epoch 116/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4068 - loss: 2.3636\n",
      "Epoch 117/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.4057 - loss: 2.3274\n",
      "Epoch 118/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4127 - loss: 2.3137\n",
      "Epoch 119/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4198 - loss: 2.3015\n",
      "Epoch 120/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.3991 - loss: 2.3607\n",
      "Epoch 121/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4189 - loss: 2.3046\n",
      "Epoch 122/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4306 - loss: 2.2748\n",
      "Epoch 123/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4282 - loss: 2.2672\n",
      "Epoch 124/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4375 - loss: 2.2389\n",
      "Epoch 125/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4349 - loss: 2.2330\n",
      "Epoch 126/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4353 - loss: 2.2042\n",
      "Epoch 127/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4374 - loss: 2.2073\n",
      "Epoch 128/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4405 - loss: 2.2191\n",
      "Epoch 129/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.4489 - loss: 2.1795\n",
      "Epoch 130/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4489 - loss: 2.1532\n",
      "Epoch 131/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4462 - loss: 2.1830\n",
      "Epoch 132/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4472 - loss: 2.1701\n",
      "Epoch 133/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4580 - loss: 2.1157\n",
      "Epoch 134/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4482 - loss: 2.1387\n",
      "Epoch 135/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4604 - loss: 2.1078\n",
      "Epoch 136/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4552 - loss: 2.1070\n",
      "Epoch 137/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4590 - loss: 2.1005\n",
      "Epoch 138/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4764 - loss: 2.0388\n",
      "Epoch 139/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4610 - loss: 2.0791\n",
      "Epoch 140/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.4644 - loss: 2.0625\n",
      "Epoch 141/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4737 - loss: 2.0430\n",
      "Epoch 142/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4800 - loss: 2.0421\n",
      "Epoch 143/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4801 - loss: 2.0130\n",
      "Epoch 144/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4799 - loss: 2.0034\n",
      "Epoch 145/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4882 - loss: 1.9927\n",
      "Epoch 146/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4793 - loss: 1.9967\n",
      "Epoch 147/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4897 - loss: 1.9540\n",
      "Epoch 148/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4921 - loss: 1.9766\n",
      "Epoch 149/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4928 - loss: 1.9478\n",
      "Epoch 150/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.4921 - loss: 1.9501\n",
      "Epoch 151/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.5032 - loss: 1.9262\n",
      "Epoch 152/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.4967 - loss: 1.9538\n",
      "Epoch 153/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5039 - loss: 1.8978\n",
      "Epoch 154/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5069 - loss: 1.8950\n",
      "Epoch 155/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5081 - loss: 1.9045\n",
      "Epoch 156/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5113 - loss: 1.8940\n",
      "Epoch 157/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5121 - loss: 1.8785\n",
      "Epoch 158/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5150 - loss: 1.8589\n",
      "Epoch 159/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5131 - loss: 1.8754\n",
      "Epoch 160/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5219 - loss: 1.8492\n",
      "Epoch 161/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5173 - loss: 1.8288\n",
      "Epoch 162/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.5208 - loss: 1.8382\n",
      "Epoch 163/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5240 - loss: 1.8206\n",
      "Epoch 164/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5286 - loss: 1.8011\n",
      "Epoch 165/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5205 - loss: 1.8334\n",
      "Epoch 166/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5372 - loss: 1.7663\n",
      "Epoch 167/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5391 - loss: 1.7846\n",
      "Epoch 168/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5258 - loss: 1.7921\n",
      "Epoch 169/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5427 - loss: 1.7530\n",
      "Epoch 170/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5352 - loss: 1.7734\n",
      "Epoch 171/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5423 - loss: 1.7294\n",
      "Epoch 172/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5442 - loss: 1.7333\n",
      "Epoch 173/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5486 - loss: 1.7463\n",
      "Epoch 174/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.5506 - loss: 1.7072\n",
      "Epoch 175/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5488 - loss: 1.7464\n",
      "Epoch 176/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5554 - loss: 1.6875\n",
      "Epoch 177/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5513 - loss: 1.6995\n",
      "Epoch 178/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5616 - loss: 1.6675\n",
      "Epoch 179/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5544 - loss: 1.6872\n",
      "Epoch 180/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5645 - loss: 1.6659\n",
      "Epoch 181/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5781 - loss: 1.6231\n",
      "Epoch 182/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5665 - loss: 1.6384\n",
      "Epoch 183/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5650 - loss: 1.6362\n",
      "Epoch 184/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5756 - loss: 1.6308\n",
      "Epoch 185/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.5770 - loss: 1.6147\n",
      "Epoch 186/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5749 - loss: 1.5969\n",
      "Epoch 187/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5733 - loss: 1.6122\n",
      "Epoch 188/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5697 - loss: 1.6106\n",
      "Epoch 189/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5730 - loss: 1.6019\n",
      "Epoch 190/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5769 - loss: 1.5691\n",
      "Epoch 191/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5943 - loss: 1.5437\n",
      "Epoch 192/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.5885 - loss: 1.5561\n",
      "Epoch 193/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5961 - loss: 1.5520\n",
      "Epoch 194/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5850 - loss: 1.5435\n",
      "Epoch 195/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.5979 - loss: 1.5137\n",
      "Epoch 196/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.5969 - loss: 1.5148\n",
      "Epoch 197/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6041 - loss: 1.5105\n",
      "Epoch 198/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6060 - loss: 1.5007\n",
      "Epoch 199/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6042 - loss: 1.4819\n",
      "Epoch 200/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.6003 - loss: 1.4933\n",
      "Epoch 201/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.6119 - loss: 1.4712\n",
      "Epoch 202/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.6110 - loss: 1.4570\n",
      "Epoch 203/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.6105 - loss: 1.4648\n",
      "Epoch 204/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.6192 - loss: 1.4263\n",
      "Epoch 205/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.6177 - loss: 1.4255\n",
      "Epoch 206/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.6103 - loss: 1.4448\n",
      "Epoch 207/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.6175 - loss: 1.4497\n",
      "Epoch 208/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.6198 - loss: 1.4367\n",
      "Epoch 209/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6170 - loss: 1.4046\n",
      "Epoch 210/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6372 - loss: 1.3716\n",
      "Epoch 211/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6335 - loss: 1.3680\n",
      "Epoch 212/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6362 - loss: 1.3711\n",
      "Epoch 213/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6247 - loss: 1.3884\n",
      "Epoch 214/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6331 - loss: 1.3800\n",
      "Epoch 215/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6400 - loss: 1.3753\n",
      "Epoch 216/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6461 - loss: 1.3253\n",
      "Epoch 217/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6469 - loss: 1.3343\n",
      "Epoch 218/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6487 - loss: 1.3165\n",
      "Epoch 219/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6410 - loss: 1.3239\n",
      "Epoch 220/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6517 - loss: 1.3057\n",
      "Epoch 221/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6487 - loss: 1.3264\n",
      "Epoch 222/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6391 - loss: 1.3318\n",
      "Epoch 223/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6505 - loss: 1.3126\n",
      "Epoch 224/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6541 - loss: 1.2981\n",
      "Epoch 225/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6523 - loss: 1.2910\n",
      "Epoch 226/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6584 - loss: 1.2634\n",
      "Epoch 227/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6654 - loss: 1.2762\n",
      "Epoch 228/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6602 - loss: 1.2665\n",
      "Epoch 229/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.6715 - loss: 1.2456\n",
      "Epoch 230/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.6654 - loss: 1.2499\n",
      "Epoch 231/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.6733 - loss: 1.2284\n",
      "Epoch 232/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6722 - loss: 1.2238\n",
      "Epoch 233/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6807 - loss: 1.2089\n",
      "Epoch 234/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6702 - loss: 1.2205\n",
      "Epoch 235/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6777 - loss: 1.1980\n",
      "Epoch 236/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6767 - loss: 1.1998\n",
      "Epoch 237/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6790 - loss: 1.1883\n",
      "Epoch 238/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6856 - loss: 1.1714\n",
      "Epoch 239/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6943 - loss: 1.1528\n",
      "Epoch 240/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6853 - loss: 1.1629\n",
      "Epoch 241/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6869 - loss: 1.1532\n",
      "Epoch 242/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6837 - loss: 1.1747\n",
      "Epoch 243/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7004 - loss: 1.1325\n",
      "Epoch 244/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7056 - loss: 1.1106\n",
      "Epoch 245/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7016 - loss: 1.1269\n",
      "Epoch 246/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7075 - loss: 1.1057\n",
      "Epoch 247/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7065 - loss: 1.0994\n",
      "Epoch 248/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7137 - loss: 1.0755\n",
      "Epoch 249/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7164 - loss: 1.0702\n",
      "Epoch 250/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7120 - loss: 1.0951\n",
      "Epoch 251/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7202 - loss: 1.0560\n",
      "Epoch 252/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7059 - loss: 1.1088\n",
      "Epoch 253/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7084 - loss: 1.0731\n",
      "Epoch 254/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7193 - loss: 1.0512\n",
      "Epoch 255/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7181 - loss: 1.0522\n",
      "Epoch 256/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7248 - loss: 1.0379\n",
      "Epoch 257/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7217 - loss: 1.0328\n",
      "Epoch 258/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.7347 - loss: 0.9975\n",
      "Epoch 259/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.7374 - loss: 0.9977\n",
      "Epoch 260/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7342 - loss: 0.9942\n",
      "Epoch 261/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7402 - loss: 0.9818\n",
      "Epoch 262/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7396 - loss: 0.9845\n",
      "Epoch 263/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7409 - loss: 0.9846\n",
      "Epoch 264/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7403 - loss: 0.9652\n",
      "Epoch 265/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7386 - loss: 0.9742\n",
      "Epoch 266/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7517 - loss: 0.9361\n",
      "Epoch 267/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7580 - loss: 0.9097\n",
      "Epoch 268/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7606 - loss: 0.9259\n",
      "Epoch 269/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7558 - loss: 0.9385\n",
      "Epoch 270/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.7558 - loss: 0.9271\n",
      "Epoch 271/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.7585 - loss: 0.9157\n",
      "Epoch 272/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7636 - loss: 0.8998\n",
      "Epoch 273/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7675 - loss: 0.8906\n",
      "Epoch 274/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7614 - loss: 0.9024\n",
      "Epoch 275/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7591 - loss: 0.9112\n",
      "Epoch 276/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7541 - loss: 0.9055\n",
      "Epoch 277/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7712 - loss: 0.8809\n",
      "Epoch 278/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7724 - loss: 0.8664\n",
      "Epoch 279/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7779 - loss: 0.8457\n",
      "Epoch 280/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7796 - loss: 0.8454\n",
      "Epoch 281/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7743 - loss: 0.8551\n",
      "Epoch 282/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7826 - loss: 0.8298\n",
      "Epoch 283/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7813 - loss: 0.8187\n",
      "Epoch 284/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.7936 - loss: 0.7981\n",
      "Epoch 285/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7965 - loss: 0.7980\n",
      "Epoch 286/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.8014 - loss: 0.7807\n",
      "Epoch 287/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.8092 - loss: 0.7727\n",
      "Epoch 288/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.8060 - loss: 0.7704\n",
      "Epoch 289/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7939 - loss: 0.7988\n",
      "Epoch 290/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.7936 - loss: 0.7818\n",
      "Epoch 291/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.8006 - loss: 0.7679\n",
      "Epoch 292/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.7989 - loss: 0.7902\n",
      "Epoch 293/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8122 - loss: 0.7358\n",
      "Epoch 294/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8137 - loss: 0.7369\n",
      "Epoch 295/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8119 - loss: 0.7246\n",
      "Epoch 296/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.8280 - loss: 0.6934\n",
      "Epoch 297/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8270 - loss: 0.6913\n",
      "Epoch 298/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.8162 - loss: 0.7223\n",
      "Epoch 299/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.8188 - loss: 0.7052\n",
      "Epoch 300/300\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.8201 - loss: 0.6877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1afe1810850>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# save the model to file\n",
    "model.save('epochBIG.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('epochBIG', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Initial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_probabilities = model.predict(pad_encoded, verbose=0)[0]  # Tüm olasılıkları al\n",
    "        \n",
    "        # Get the index of the word with the highest probability\n",
    "        pred_word_ind = np.argmax(pred_probabilities)  # En yüksek olasılıklı kelimenin indeksi\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab a random seed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahmin = generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerçek = ' '.join(text_sequences[9547])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thought',\n",
       " 'i',\n",
       " 'to',\n",
       " 'myself',\n",
       " 'the',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'human',\n",
       " 'being',\n",
       " 'just',\n",
       " 'as',\n",
       " 'i',\n",
       " 'am',\n",
       " 'he',\n",
       " 'has',\n",
       " 'just',\n",
       " 'as',\n",
       " 'much',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'fear',\n",
       " 'me',\n",
       " 'as',\n",
       " 'i',\n",
       " 'have']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_sequences[9521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9521"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[9521])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have to be afraid of him better sleep with a sober cannibal than a drunken christian landlord said i tell him to stash his tomahawk'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[9547])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(real_text, generated_text):\n",
    "    # Kelimeleri liste olarak ayır\n",
    "    real_words = real_text.split()\n",
    "    generated_words = generated_text.split()\n",
    "    \n",
    "    # Kelime sayısını eşitle\n",
    "    min_length = min(len(real_words), len(generated_words))\n",
    "    \n",
    "    # Doğru tahmin edilen kelimeleri say\n",
    "    correct_predictions = sum(1 for i in range(min_length) if real_words[i] == generated_words[i])\n",
    "    \n",
    "    # Doğruluğu hesapla\n",
    "    accuracy = correct_predictions / min_length\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğruluk: 0.27\n"
     ]
    }
   ],
   "source": [
    "# Doğruluğu hesapla\n",
    "accuracy = calculate_accuracy(gerçek, tahmin)\n",
    "print(f\"Doğruluk: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Skoru: 0.23\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu_score(real_text, generated_text):\n",
    "    real_words = real_text.split()\n",
    "    generated_words = generated_text.split()\n",
    "    score = sentence_bleu([real_words], generated_words)\n",
    "    return score\n",
    "\n",
    "bleu_score = calculate_bleu_score(gerçek, tahmin)\n",
    "print(f\"BLEU Skoru: {bleu_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAT BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 'rb' (read binary) modunda dosyayı açın\n",
    "with open(r'C:\\Users\\Pc\\Desktop\\train_qa.txt', 'rb') as fp:   # Unpickling\n",
    "    train_data = pickle.load(fp)\n",
    "\n",
    "with open(r'C:\\Users\\Pc\\Desktop\\test_qa.txt', 'rb') as fp:   # Unpickling\n",
    "    test_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Vocabulary of All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set that holds the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    # In case you don't know what a union of sets is:\n",
    "    # https://www.programiz.com/python-programming/methods/set/union\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Python'da set() fonksiyonu, kendisine verilen girdiyi öğelerine ayırarak her bir öğeyi bir küme elemanı olarak ekler. Girdi bir kelime (string) ise, her bir harfi ayrı bir öğe olarak kümeye ekler. Girdi bir cümle ya da kelime listesi ise, her bir kelimeyi öğe olarak ekler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras'ın pad_sequences fonksiyonu ile çalışırken, modelinizin eğitilebilmesi için dizileri aynı uzunlukta yapmanız gerekir. Ancak, bazı diziler daha kısa olabileceğinden, bu dizilere \"0\" ile dolgu (padding) yapmanız gerekir. Bu durumda, 0 genellikle özel bir anlam taşır ve dizilerdeki eksik veriyi ya da başlangıç boşluklarını belirtir. İşte bu yüzden bir 0 değeri tutmak için ekstra bir alan eklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'went': 1,\n",
       " 'moved': 2,\n",
       " 'is': 3,\n",
       " 'bathroom': 4,\n",
       " 'dropped': 5,\n",
       " 'travelled': 6,\n",
       " 'the': 7,\n",
       " 'milk': 8,\n",
       " 'put': 9,\n",
       " '?': 10,\n",
       " 'left': 11,\n",
       " 'garden': 12,\n",
       " 'discarded': 13,\n",
       " 'kitchen': 14,\n",
       " 'apple': 15,\n",
       " 'office': 16,\n",
       " 'up': 17,\n",
       " 'hallway': 18,\n",
       " 'in': 19,\n",
       " 'john': 20,\n",
       " 'to': 21,\n",
       " 'daniel': 22,\n",
       " 'mary': 23,\n",
       " 'took': 24,\n",
       " 'back': 25,\n",
       " 'journeyed': 26,\n",
       " 'sandra': 27,\n",
       " 'got': 28,\n",
       " 'picked': 29,\n",
       " 'down': 30,\n",
       " '.': 31,\n",
       " 'football': 32,\n",
       " 'no': 33,\n",
       " 'yes': 34,\n",
       " 'grabbed': 35,\n",
       " 'there': 36,\n",
       " 'bedroom': 37}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalize Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories,Queries,and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
    "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
    "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [tokenizer.word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [tokenizer.word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        y = np.zeros(len(tokenizer.word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n",
    "        #\n",
    "        y[tokenizer.word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story,query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'John', 'in', 'the', 'kitchen', '?']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 20, 19,  7, 14, 10],\n",
       "       [ 3, 20, 19,  7, 14, 10],\n",
       "       [ 3, 20, 19,  7, 12, 10],\n",
       "       ...,\n",
       "       [ 3, 23, 19,  7, 37, 10],\n",
       "       [ 3, 27, 19,  7, 12, 10],\n",
       "       [ 3, 23, 19,  7, 12, 10]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       503., 497.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, Reshape\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The total size of the tensor must be unchanged. Received: input_shape=(156, 6), target_shape=(156, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# match tensörünü (samples, story_maxlen, 1) olacak şekilde yeniden şekillendir\u001b[39;00m\n\u001b[0;32m     27\u001b[0m match \u001b[38;5;241m=\u001b[39m Permute((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))(match)  \u001b[38;5;66;03m# (samples, query_maxlen, story_maxlen) \u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m match \u001b[38;5;241m=\u001b[39m Reshape((max_story_len, \u001b[38;5;241m1\u001b[39m))(match)  \u001b[38;5;66;03m# (samples, story_maxlen, 1)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Cevapları oluşturmak için benzerlik matrisini ve hikaye gömme dizisini birleştir\u001b[39;00m\n\u001b[0;32m     31\u001b[0m response \u001b[38;5;241m=\u001b[39m add([match, input_encoded_m])  \u001b[38;5;66;03m# (samples, story_maxlen, embedding_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation_utils.py:302\u001b[0m, in \u001b[0;36mcompute_reshape_output_shape\u001b[1;34m(input_shape, newshape, newshape_arg_name)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unknown_dim_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_size \u001b[38;5;241m!=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(newshape):\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total size of the tensor must be unchanged. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewshape_arg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m newshape\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# We have one -1 in `newshape`, compute the actual value\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The total size of the tensor must be unchanged. Received: input_shape=(156, 6), target_shape=(156, 1)"
     ]
    }
   ],
   "source": [
    "# Giriş katmanları\n",
    "input_sequence = Input(shape=(max_story_len,))\n",
    "question = Input(shape=(max_question_len,))\n",
    "\n",
    "# Hikaye girişi için gömme katmanı (embedding layer)\n",
    "input_encoder_m = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64),  # Her kelimeyi 64 boyutlu vektörlere dönüştür\n",
    "    Dropout(0.3)  # Aşırı öğrenmeyi önlemek için dropout\n",
    "])\n",
    "\n",
    "# Soru girişi için gömme katmanı\n",
    "question_encoder = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_question_len),\n",
    "    Dropout(0.3)  # Aşırı öğrenmeyi önlemek için dropout\n",
    "])\n",
    "\n",
    "# Hikaye ve soru girdi dizilerini vektör dizilerine dönüştür\n",
    "input_encoded_m = input_encoder_m(input_sequence)  # (samples, story_maxlen, embedding_dim)\n",
    "question_encoded = question_encoder(question)  # (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# Benzerlik matrisini hesapla\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))  # (samples, story_maxlen, query_maxlen)\n",
    "# Benzerlik matrisini softmax ile normalize et\n",
    "match = Activation('softmax')(match)  # (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# match tensörünü (samples, story_maxlen, 1) olacak şekilde yeniden şekillendir\n",
    "match = Permute((1, 2))(match)  # (samples, query_maxlen, story_maxlen) \n",
    "match = Reshape((max_story_len, 1))(match)  # (samples, story_maxlen, 1)\n",
    "\n",
    "# Cevapları oluşturmak için benzerlik matrisini ve hikaye gömme dizisini birleştir\n",
    "response = add([match, input_encoded_m])  # (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# Soru vektör dizisi ile yan yana ekle\n",
    "answer = concatenate([response, question_encoded])  # (samples, query_maxlen, embedding_dim * 2)\n",
    "\n",
    "# Cevap dizisini LSTM ile işleme\n",
    "answer = LSTM(32)(answer)  # (samples, 32) boyutuna getir\n",
    "answer = Dropout(0.5)(answer)  # Aşırı öğrenmeyi önlemek için dropout\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size) boyutuna getir\n",
    "answer = Activation('softmax')(answer)  # Kelime dağarcığı üzerinde olasılık dağılımı elde et\n",
    "\n",
    "# Modeli oluştur\n",
    "model = Model(inputs=[inputs_train, queries_train], outputs=answers_train)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
